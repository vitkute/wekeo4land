{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../media/common/LogoWekeo_Copernicus_RGB_0.png' align='left' height='96px'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grassland Classification\n",
    "\n",
    "*Authors: Adrian Di Paolo, Chung-Xiang Hong, Jonas Viehweger* \n",
    "\n",
    "This notebook will demonstrate how to clean up and pre process satellite data for a machine learning task. It will also go into detail on how to use the cleaned data to train, evaluate and select a model for a larger scale application. \n",
    "\n",
    "The task is to train a model which can classify grassland areas in the Netherlands. The ultimate goal would be to classify grasslands yearly to derive change maps of grassland loss and gain from them. To do the classification we are using phenological data and the EuroCrops Dataset as ground truth.\n",
    "\n",
    "This notebook uses data that was already downloaded and prepared for Python in the previous notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Preprocessing Data](#preprocessing-data)\n",
    "\n",
    "    - [Labeling Data](#labeling-the-data)\n",
    "\n",
    "    - [Data Cleaning](#data-cleaning)\n",
    "\n",
    "    - [Normalizing Dates](#normalizing-dates)\n",
    "\n",
    "    - [Normalizing Numerical Data](#normalize-numerical-data)\n",
    "\n",
    "    - [Split Dataset into Train and Test](#split-the-dataset-into-train-and-test) \n",
    "\n",
    "2. [Model Training](#model-training)\n",
    "\n",
    "    \n",
    "3. [Model Evaluation](#model-evaluation)\n",
    "\n",
    "    - [Metrics](#metrics)\n",
    "\n",
    "    - [Memory Usage](#memory-usage)\n",
    "\n",
    "    - [Execution Time](#execution-time) \n",
    "\n",
    "    - [Confusion Matrix](#confusion-matrtix)\n",
    "\n",
    "    - [ROC Curve](#roc-curve)\n",
    "    \n",
    "4. [Model Selection](#model-selection)\n",
    "\n",
    "    - [Fine-tuning](#hyperparameter-optimization)\n",
    "\n",
    "    - [Feature Importance](#feature-importance)\n",
    "\n",
    "5. [Neural Network](#bonus-training-neural-network)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(dplyr)\n",
    "library(lubridate)\n",
    "library(caret)\n",
    "library(naivebayes)\n",
    "library(lightgbm)\n",
    "library(doParallel)\n",
    "library(scales)\n",
    "library(glmnet)\n",
    "\n",
    "library(ggplot2)\n",
    "library(gridExtra)\n",
    "library(pROC)\n",
    "\n",
    "library(MLmetrics)\n",
    "library(keras)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Load the downloaded data\n",
    "x_data <- readRDS('../../data/processing/ml-grassland-classification/dataset/x_data.rds')\n",
    "y_data <- readRDS('../../data/processing/ml-grassland-classification/dataset/y_data.rds')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labeling the data\n",
    "\n",
    "We assign a 1 to the grassland label and 0 for others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "binary_label <- function(y_data) {\n",
    "    # Assign new labels: 1 if grassland, else 0\n",
    "    grassland_value_str <- format(3.302e+09, scientific = TRUE)\n",
    "\n",
    "    # Convert the array to strings in scientific notation\n",
    "    y_data_str <- sapply(y_data, format, scientific = TRUE)\n",
    "\n",
    "    binary_assign <- function(x) {\n",
    "        ifelse(x == grassland_value_str, 1, 0)\n",
    "    }\n",
    "\n",
    "    sapply(y_data_str, binary_assign, USE.NAMES = FALSE)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning\n",
    "\n",
    "In our dataset we have Nan values or values that indicate No Data in the HRVPP documentation, so we delete the rows containing those values from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "data_to_df <- function(x_data, y_data = NULL) {\n",
    "\n",
    "    # Transform the data to dataframe and cleans it\n",
    "    x_df <- data.frame(x_data)\n",
    "    colnames(x_df) <- c('AMPL', 'EOSD', 'EOSV', 'LENGTH', 'LSLOPE', 'MAXD', 'MAXV', 'MINV', 'QFLAG', 'RSLOPE', 'SOSD', 'SOSV', 'SPROD', 'TPROD')\n",
    "\n",
    "    if (!is.null(y_data)) {\n",
    "        y_df <- data.frame(LABEL = y_data)\n",
    "        df <- cbind(x_df, y_df)\n",
    "    } else {\n",
    "        df <- x_df\n",
    "    }\n",
    "    \n",
    "    df <- df[complete.cases(df), ]\n",
    "    df <- df[!(df$EOSD == 0 | df$SOSD == 0 | df$MAXD == 0 | df$LENGTH == 0), ]\n",
    "    df <- df[!(df$SOSV == 32768 | df$EOSV == 32768 | df$MAXV == 32768 | df$MINV == 32768 | df$AMPL == 32768 | df$LSLOPE == 32768 | df$RSLOPE == 32768), ]\n",
    "    df <- df[!(df$SPROD == 65535 | df$TPROD == 65535), ]\n",
    "    \n",
    "    return(df)\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing Dates\n",
    "\n",
    "As we are dealing with datasets that include columns with dates, it is essential to ensure that those columns are normalized. \n",
    "\n",
    "In our case, we have a dataset for different years that come with dates in the format 'YYDOY'. So the first two digits representing the year, and the last three digits representing the day of the year. As we need those dates to be consistent with other years, and also be in the same range, we transform them to a number representing the count of days since a reference date (1st of January of the previous year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "transform_dates <- function(df) {\n",
    "    # Transform dates columns from YYDOY to YY-MM-DD and then to the days since the 1 January of the previous year\n",
    "    \n",
    "    df$EOSD <- as.character(df$EOSD)\n",
    "    df$SOSD <- as.character(df$SOSD)\n",
    "    df$MAXD <- as.character(df$MAXD)\n",
    "    \n",
    "    df$SOSD <- as.Date(df$SOSD, format = \"%y%j\")\n",
    "    df$EOSD <- as.Date(df$EOSD, format = \"%y%j\")\n",
    "    df$MAXD <- as.Date(df$MAXD, format = \"%y%j\")\n",
    "    \n",
    "    min_year <- min(format(df$SOSD, \"%Y\"))\n",
    "    reference_date <- as.Date(paste0(min_year, \"-01-01\"))\n",
    "    \n",
    "    df$SOSD <- as.numeric(df$SOSD - reference_date)\n",
    "    df$EOSD <- as.numeric(df$EOSD - reference_date)\n",
    "    df$MAXD <- as.numeric(df$MAXD - reference_date)\n",
    "    \n",
    "    return(df)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Normalizing Numerical Data\n",
    "\n",
    "To ensure that our machine learning models perform optimally, it is essential to rescale the numerical data. Rescaling, transforms the data to a common scale without distorting differences in the ranges of values. This process is crucial for algorithms that compute distances between data points, such as gradient boosting and neural networks, as it ensures that features with larger ranges do not dominate the learning process.  \n",
    "\n",
    "In this notebook, we will apply Min-Max scaling to bring all numerical features into the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "normalize_df <- function(df) {\n",
    "    \n",
    "    columns_to_scale <- df %>%\n",
    "        select(-LABEL, -MAXD, -SOSD, -EOSD)\n",
    "    \n",
    "    scaled_data <- as.data.frame(lapply(columns_to_scale, rescale))\n",
    "    \n",
    "    df <- df %>%\n",
    "        select(LABEL, MAXD, SOSD, EOSD) %>%\n",
    "        bind_cols(scaled_data)\n",
    "    \n",
    "    return(df)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for the Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "y_data <- binary_label(y_data)\n",
    "\n",
    "df_data <- data_to_df(x_data, y_data) %>%\n",
    "    transform_dates() %>%\n",
    "    normalize_df()\n",
    "\n",
    "x_data <- df_data %>% select(-LABEL)\n",
    "y_data <- df_data$LABEL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split Dataset into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(42)\n",
    "train_index <- createDataPartition(y_data, p = .8, list = FALSE, times = 1)\n",
    "\n",
    "X_train <- x_data[train_index, ]\n",
    "X_test <- x_data[-train_index, ]\n",
    "y_train <- y_data[train_index]\n",
    "y_test <- y_data[-train_index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersampling Dataset\n",
    "\n",
    "Undersampling is a technique used to balance imbalanced datasets, where one class has significantly more samples than another class.\n",
    "\n",
    "The main advantage of undersampling is that it can improve the performance of classifiers by reducing the bias towards the majority class, which can lead to better predictions on the minority class. Undersampling can also reduce the training time and memory requirements of the model, since there are fewer instances to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Compute class distribution before undersampling\n",
    "class_counts_before <- table(y_train)\n",
    "print(class_counts_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "undersampled_data <- downSample(x = X_train, y = as.factor(y_train), list = TRUE)\n",
    "\n",
    "X_train <- undersampled_data$x\n",
    "y_train <- undersampled_data$y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "class_counts_after <- table(y_train)\n",
    "\n",
    "# Convert to data frames for ggplot\n",
    "class_counts_before_df <- as.data.frame(class_counts_before)\n",
    "colnames(class_counts_before_df) <- c(\"Class\", \"Count\")\n",
    "class_counts_before_df$Class <- factor(class_counts_before_df$Class, levels = c(\"1\", \"0\"), labels = c(\"Grassland\", \"No Grassland\"))\n",
    "class_counts_before_df$Percent <- round(class_counts_before_df$Count / sum(class_counts_before_df$Count) * 100, 1)\n",
    "\n",
    "\n",
    "class_counts_after_df <- as.data.frame(class_counts_after)\n",
    "colnames(class_counts_after_df) <- c(\"Class\", \"Count\")\n",
    "class_counts_after_df$Class <- factor(class_counts_after_df$Class, levels = c(\"1\", \"0\"), labels = c(\"Grassland\", \"No Grassland\"))\n",
    "class_counts_after_df$Percent <- round(class_counts_after_df$Count / sum(class_counts_after_df$Count) * 100, 1)\n",
    "\n",
    "\n",
    "# Create pie chart for class distribution before undersampling\n",
    "plot_before <- ggplot(class_counts_before_df, aes(x = \"\", y = Count, fill = Class)) +\n",
    "  geom_bar(stat = \"identity\", width = 1) +\n",
    "  coord_polar(theta = \"y\") +\n",
    "  scale_fill_manual(values = c(\"Grassland\" = \"#4CAF50\", \"No Grassland\" = \"#B0B0B0\")) +\n",
    "  theme_void() +\n",
    "  theme(legend.position = \"right\") +\n",
    "  geom_text(aes(label = paste0(Percent, \"%\")), position = position_stack(vjust = 0.5)) +\n",
    "  ggtitle(\"Class Distribution Before Undersampling\")\n",
    "\n",
    "# Create pie chart for class distribution after undersampling\n",
    "plot_after <- ggplot(class_counts_after_df, aes(x = \"\", y = Count, fill = Class)) +\n",
    "  geom_bar(stat = \"identity\", width = 1) +\n",
    "  coord_polar(theta = \"y\") +\n",
    "  scale_fill_manual(values = c(\"Grassland\" = \"#4CAF50\", \"No Grassland\" = \"#B0B0B0\")) +\n",
    "  theme_void() +\n",
    "  theme(legend.position = \"right\") +\n",
    "  geom_text(aes(label = paste0(Percent, \"%\")), position = position_stack(vjust = 0.5)) +\n",
    "  ggtitle(\"Class Distribution After Undersampling\")\n",
    "\n",
    "options(repr.plot.width = 24, repr.plot.height = 8)\n",
    "grid.arrange(plot_before, plot_after, ncol = 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "Here we are testing quite a few different algorithms on their performance for the task. \n",
    "It has to be noted that this is a quite naive and brute force approach to the task, since the models hyperparameters aren't tweaked and no pre-selection of machine learning algorithms based on expert knowledge is made. \n",
    "\n",
    "However it will give a rough idea on the performance of the algorithms and in addition it will provide information on the computational efficiency of the algorithms in terms of memory usage and computation time. These are also important parameters to consider when scaling the model up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Shuffling the data is crucial for gradient boosting models like LightGBM. It ensures that the training\n",
    "# and validation sets are representative of the overall dataset, which helps the model learn generalizable\n",
    "# patterns. This prevents biases and overfitting to the order of the data, improving the robustness and\n",
    "# performance of the gradient boosting process.\n",
    "\n",
    "set.seed(123) # For reproducibility\n",
    "\n",
    "data <- data.frame(X_train, LABEL = y_train)\n",
    "shuffled_data <- data[sample(nrow(data)), ]\n",
    "\n",
    "X_train_shuffled <- shuffled_data %>% select(-LABEL)\n",
    "y_train_shuffled <- shuffled_data$LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "models <- {}\n",
    "\n",
    "# Metrics initialization\n",
    "accuracy <- list()\n",
    "precision <- list()\n",
    "recall <- list()#\n",
    "f1 <- list()\n",
    "time_usage <- list()\n",
    "\n",
    "# Logistic Regression\n",
    "start_time <- Sys.time()\n",
    "models[[\"Logistic Regression\"]] <- train(X_train_shuffled, y_train_shuffled,\n",
    "  method = \"glmnet\",\n",
    "  trControl = trainControl(method = \"cv\", number = 10),\n",
    "  tuneGrid = expand.grid(alpha = 0, lambda = seq(0.001, 0.1, by = 0.001))\n",
    ")\n",
    "time_usage[[\"Logistic Regression\"]] <- as.numeric(difftime(Sys.time(), start_time, units = \"secs\"))\n",
    "\n",
    "# Support Vector Machines\n",
    "start_time <- Sys.time()\n",
    "models[[\"Support Vector Machines\"]] <- train(X_train_shuffled, y_train_shuffled,\n",
    "  #data = train_data,\n",
    "  method = \"svmLinear\",\n",
    "  trControl = trainControl(method = \"cv\", number = 10, allowParallel = TRUE)\n",
    ")\n",
    "time_usage[[\"Support Vector Machines\"]] <- as.numeric(difftime(Sys.time(), start_time, units = \"secs\"))\n",
    "\n",
    "# Decision Trees\n",
    "start_time <- Sys.time()\n",
    "models[[\"Decision Trees\"]] <- train(X_train_shuffled, y_train_shuffled,\n",
    "  # data = train_data,\n",
    "  method = \"rpart\",\n",
    "  trControl = trainControl(method = \"cv\", number = 10, allowParallel = TRUE),\n",
    "  tuneLength = 10\n",
    ")\n",
    "time_usage[[\"Decision Trees\"]] <- as.numeric(difftime(Sys.time(), start_time, units = \"secs\"))\n",
    "\n",
    "\n",
    "# Naive Bayes\n",
    "start_time <- Sys.time()\n",
    "models[[\"Naive Bayes\"]] <- train(X_train_shuffled, y_train_shuffled,\n",
    "  # data = train_data,\n",
    "  method = \"naive_bayes\",\n",
    "  trControl = trainControl(method = \"cv\", number = 10, allowParallel = TRUE),\n",
    "  tuneLength = 10\n",
    ")\n",
    "time_usage[[\"Naive Bayes\"]] <- as.numeric(difftime(Sys.time(), start_time, units = \"secs\"))\n",
    "\n",
    "# K-Nearest Neighbors\n",
    " start_time <- Sys.time()\n",
    "models[[\"K-Nearest Neighbor\"]] <- train(X_train_shuffled, y_train_shuffled,\n",
    "  #  data = train_data,\n",
    "   method = \"knn\",\n",
    "   trControl = trainControl(method = \"cv\", number = 10, allowParallel = TRUE),\n",
    "   #tuneGrid = expand.grid(k = c(seq(5, 15, by = 2))),  # Include k = 1\n",
    "   preProcess = c(\"center\", \"scale\")  # Ensure data is scaled\n",
    " )\n",
    " time_usage[[\"K-Nearest Neighbor\"]] <- as.numeric(difftime(Sys.time(), start_time, units = \"secs\"))\n",
    "\n",
    "\n",
    "# LightGBM\n",
    "#\n",
    "train_lightgbm <- function(x_train, y_train, custom_params = list()) {\n",
    "\n",
    "  params <- list(\n",
    "    objective = \"binary\",\n",
    "    metric = \"binary_error\",\n",
    "    feature_fraction = 1,\n",
    "    learning_rate = 0.02,\n",
    "    num_leaves = 25\n",
    "  )\n",
    "\n",
    "  params <- modifyList(params, custom_params)\n",
    "\n",
    "  lgb.train(\n",
    "    params = params,\n",
    "    data = lgb.Dataset(data = as.matrix(x_train), label = as.numeric(as.character(y_train))),\n",
    "    nrounds = 100,\n",
    "    verbose = 0\n",
    "  )\n",
    "\n",
    "}\n",
    "\n",
    "start_time <- Sys.time()\n",
    "models[[\"Lightgbm\"]] <- train_lightgbm(X_train_shuffled, y_train_shuffled)\n",
    "time_usage[[\"Lightgbm\"]] <- as.numeric(difftime(Sys.time(), start_time, units = \"secs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "for (key in names(models)) {\n",
    "\n",
    "  print(key)\n",
    "  flush.console()\n",
    "\n",
    "  cm <- NULL\n",
    "  if (key == \"Lightgbm\") {\n",
    "    predictions <- predict(models[[key]], as.matrix(X_test))\n",
    "    predictions_binary <- ifelse(predictions > 0.5, 1, 0)\n",
    "    cm <- caret::confusionMatrix(factor(predictions_binary), factor(y_test))\n",
    "  } else {\n",
    "    predictions <- predict(models[[key]], X_test)\n",
    "    cm <- caret::confusionMatrix(predictions, factor(y_test))\n",
    "  }\n",
    "\n",
    "  accuracy[[key]] <- cm$overall[\"Accuracy\"]\n",
    "  precision[[key]] <- cm$byClass[\"Precision\"]\n",
    "  recall[[key]] <- cm$byClass[\"Recall\"]\n",
    "  f1[[key]] <- cm$byClass[\"F1\"]\n",
    "\n",
    "  print(paste(\"F1: \", (f1[[key]])))\n",
    "\n",
    "  # Print time usage\n",
    "  cat(sprintf(\"Model: %s\\nTime: %.2f s\\n\\n\", key, time_usage[[key]]))\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot_dict <- function(data, title, x_label, y_label) {\n",
    "  df <- data.frame(metric = unlist(data), model = names(data))\n",
    "\n",
    "  # Sort the DataFrame by the metric column\n",
    "  df$model <- factor(df$model, levels = df$model[order(-df$metric)])\n",
    "\n",
    "  # Create a color palette for the bars\n",
    "  colors <- scale_fill_gradient(low = \"lightgreen\", high = \"darkgreen\")\n",
    "\n",
    "  # Create the plot\n",
    "  ggplot(df, aes(x = metric, y = model)) +\n",
    "    geom_bar(stat = \"identity\", aes(fill = metric), show.legend = FALSE) +\n",
    "    colors +\n",
    "    theme_minimal() +\n",
    "    labs(title = title, x = x_label, y = y_label) +\n",
    "    theme(\n",
    "      plot.title = element_text(hjust = 0.5, size = 20),\n",
    "      axis.title.x = element_text(size = 16),\n",
    "      axis.title.y = element_text(size = 16),\n",
    "      axis.text = element_text(size = 14)\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot_dict(f1, 'Models F1-Score', 'F1-Score', 'Model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execution Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot_dict(time_usage, 'Training Execution Time', 'Time (s)', 'Model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the Lightgbm model is by far the more efficient."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Validation Dataset\n",
    "\n",
    "Now we will prove this model with the validation dataset, which was taken from a different bounding box area. This will give us the performance of the model on data which hasn't been seen during the training. If the performance of the model is much worse for this dataset, it means that the model has been overfit on the training data and isn't general enough to get a good performance on new data.\n",
    "\n",
    "We also have to pre-process the validation data them with the sane steps as explained before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x_validation <- readRDS('../../data/processing/ml-grassland-classification/dataset/x_validation.rds')\n",
    "y_validation <- readRDS('../../data/processing/ml-grassland-classification/dataset/y_validation.rds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "y_validation <- binary_label(y_validation)\n",
    "\n",
    "df_validation <- data_to_df(x_validation, y_validation) %>%\n",
    "    transform_dates() %>%\n",
    "    normalize_df()\n",
    "\n",
    "x_validation <- df_validation %>% select(-LABEL)\n",
    "y_validation <- df_validation$LABEL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating the models performance in the validation dataset, we will use confusion matrices and ROC curves. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "\n",
    "A confusion matrix is a table that summarizes the performance of a classification model by comparing the predicted labels with the true labels of a set of data. \n",
    "\n",
    "The confusion matrix consists of four values: true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN). The rows of the matrix represent the actual labels, while the columns represent the predicted labels. The diagonal elements of the matrix represent the instances that are classified correctly, while the off-diagonal elements represent the instances that are misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrices <- function(models, X_test, y_test) {\n",
    "    num_models <- length(models)\n",
    "    nrows <- ceiling(num_models / 3)\n",
    "    ncols <- min(num_models, 3)\n",
    "    plots <- list()\n",
    "\n",
    "    class_names <- c(\"Grassland\", \"No Grassland\")\n",
    "    references <- factor(y_test)\n",
    "\n",
    "    for (key in names(models)) {\n",
    "        print(key)\n",
    "        model <- models[[key]]\n",
    "\n",
    "        predictions <- predict(model, as.matrix(X_test))\n",
    "\n",
    "        if (key == \"Lightgbm\") {\n",
    "            predictions_binary <- ifelse(predictions > 0.9, 1, 0)\n",
    "            cm <- confusionMatrix(factor(predictions_binary), references)\n",
    "        } else {\n",
    "            cm <- confusionMatrix(predictions, references)\n",
    "        }\n",
    "\n",
    "        cm_normalized <- prop.table(cm$table, 1)\n",
    "        cm_data <- as.data.frame(cm_normalized)\n",
    "        colnames(cm_data) <- c(\"True\", \"Predicted\", \"Freq\")\n",
    "        cm_data$Freq[is.na(cm_data$Freq)] <- 0\n",
    "\n",
    "        p <- ggplot(cm_data, aes(y = Predicted, x = True, fill = Freq)) +\n",
    "            coord_equal() +\n",
    "            geom_tile() +\n",
    "            geom_text(aes(label = sprintf(\"%.2f\", Freq))) +\n",
    "            scale_fill_gradient(low = \"white\", high = \"darkgreen\") +\n",
    "            scale_x_discrete(labels = class_names) +\n",
    "            scale_y_discrete(labels = class_names) +\n",
    "            labs(title = key, y = \"True labels\", x = \"Predicted labels\", ) +\n",
    "            theme_minimal() +\n",
    "            theme(legend.position = \"none\")\n",
    "\n",
    "        plots[[key]] <- p\n",
    "    }\n",
    "\n",
    "    grid.arrange(grobs = plots, nrow = nrows, ncol = ncols)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrices(models, x_validation, y_validation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve\n",
    "\n",
    "The ROC (Receiver Operating Characteristic) curve is a graphical representation of the performance of a binary classifier. The ROC curve shows the trade-off between the true positive rate (TPR), also called sensitivity or recall, and the false positive rate (FPR), which is the proportion of negative instances that are incorrectly classified as positive.\n",
    "\n",
    "To create a ROC curve, the classifier's output is sorted by confidence or probability, and the threshold for classification is varied from high to low. At each threshold value, the TPR and FPR are calculated and plotted on a graph with TPR on the y-axis and FPR on the x-axis. The resulting curve represents the classifier's performance at all possible threshold values.\n",
    "\n",
    "The closer the curve is to the top-left corner of the graph, the better the classifier's performance, as this indicates a high TPR and a low FPR. The area under the ROC curve (AUC) is a commonly used metric to summarize the classifier's performance. A perfect classifier would have an AUC of 1, while a random classifier would have an AUC of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot_roc_curve <- function(models, x_test, y_test, labels) {\n",
    "  roc_data <- data.frame()\n",
    "\n",
    "  for (i in seq_along(models)) {\n",
    "    model <- models[[i]]\n",
    "    label <- labels[i]\n",
    "\n",
    "    y_prob <- predict(model, as.matrix(x_test))\n",
    "    y_prob <- as.numeric(as.character(y_prob))\n",
    "    roc_curve <- roc(y_test, y_prob)\n",
    "\n",
    "    roc_auc <- auc(roc_curve)\n",
    "    roc_df <- data.frame(\n",
    "      fpr = 1 - roc_curve$specificities,\n",
    "      tpr = roc_curve$sensitivities,\n",
    "      model = paste(label, \"(AUC =\", sprintf(\"%.2f\", roc_auc), \")\")\n",
    "    )\n",
    "\n",
    "    roc_data <- rbind(roc_data, roc_df)\n",
    "  }\n",
    "\n",
    "  ggplot(roc_data, aes(x = fpr, y = tpr, color = model)) +\n",
    "    geom_line(linewidth = 1) +\n",
    "    geom_abline(linetype = \"dashed\", color = \"gray\") +\n",
    "    xlim(0, 1) +\n",
    "    ylim(0, 1) +\n",
    "    labs(x = \"False Positive Rate\", y = \"True Positive Rate\", title = \"ROC Curve\") +\n",
    "    theme_minimal() +\n",
    "    theme(\n",
    "      legend.position = \"bottom\",\n",
    "      plot.title = element_text(size = 20),\n",
    "      axis.title = element_text(size = 16),\n",
    "      axis.text = element_text(size = 14),\n",
    "      legend.text = element_text(size = 12),\n",
    "      legend.title = element_text(size = 14)\n",
    "    ) +\n",
    "    guides(color = guide_legend(title = \"Models\"))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x_validation[is.na(x_validation)] <- -1\n",
    "\n",
    "model_labels <- names(models)\n",
    "plot_roc_curve(models, x_validation, y_validation, model_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "\n",
    "As we can see for the previous metrics, memory and time usage, the best candidate to solve this binary classification problem is the LightGBM model.\n",
    "\n",
    "LightGBM is a popular open-source gradient boosting framework that was developed by Microsoft. It is designed to be highly efficient in terms of training speed and memory usage, making it a popular choice for large-scale machine learning tasks. LightGBM uses gradient boosting algorithms to build models, which iteratively improves the performance of a weak learner by adding new decision trees to the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "lgbm <- models[[\"Lightgbm\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter Optimization\n",
    "\n",
    "GridSearchCV is a technique used to fine-tune the hyperparameters in order to improve its performance. In essence, it involves searching over a range of values for each hyperparameter and finding the combination that yields the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "f1_metric <- function(preds, dtrain) {\n",
    "  labels <- dtrain$construct()$get_field(\"label\")\n",
    "\n",
    "  predictions_binary <- ifelse(preds > 0.9, 1, 0)\n",
    "  cm <- caret::confusionMatrix(factor(predictions_binary), factor(labels))\n",
    "  f1 <- cm$byClass[\"F1\"]\n",
    "\n",
    "  return(list(name = \"f1\", value = f1, higher_better = TRUE))\n",
    "}\n",
    "\n",
    "# Define the parameter grid\n",
    "grid_params <- expand.grid(\n",
    "  learning_rate = c(0.1, 0.01),\n",
    "  num_leaves = c(20, 30),\n",
    "  max_depth = c(7, 10, 14)\n",
    ")\n",
    "\n",
    "# Initialize variables to store the best results\n",
    "best_params <- NULL\n",
    "best_f1 <- -Inf\n",
    "\n",
    "# Grid search loop\n",
    "for (i in 1:nrow(grid_params)) {\n",
    "  params <- list(\n",
    "    objective = \"binary\",\n",
    "    metric = \"None\",  # No built-in metric, using custom metric\n",
    "    learning_rate = grid_params$learning_rate[i],\n",
    "    num_leaves = grid_params$num_leaves[i],\n",
    "    max_depth = grid_params$max_depth[i]\n",
    "  )\n",
    "  \n",
    "  \n",
    "  # Perform cross-validation with the custom F1 metric\n",
    "  cv_result <- lgb.cv(\n",
    "    params = params,\n",
    "    data = lgb.Dataset(data = as.matrix(X_train), label = as.numeric(as.character(y_train))),\n",
    "    nfold = 5,\n",
    "    eval = f1_metric,\n",
    "    verbose = 0\n",
    "  )\n",
    "  \n",
    "  best_f1_iter <- max(cv_result$best_score)\n",
    "  \n",
    "  # Update best parameters if current F1 score is higher\n",
    "  if (best_f1_iter > best_f1) {\n",
    "    best_f1 <- best_f1_iter\n",
    "    best_params <- params\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Print the best parameters and the corresponding F1 score\n",
    "print(\"Best parameters:\")\n",
    "print(best_params)\n",
    "\n",
    "lgbm <- train_lightgbm(X_train, y_train, custom_params = best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_lgbm <- function(model, x_test, y_test) {\n",
    "\n",
    "  # Make predictions\n",
    "  predictions <- predict(model, as.matrix(x_test))\n",
    "  predictions_binary <- ifelse(predictions > 0.9, 1, 0)\n",
    "\n",
    "\n",
    "  # Calculate the confusion matrix using the caret package\n",
    "  cm <- caret::confusionMatrix(factor(predictions_binary), factor(y_test))\n",
    "\n",
    "  # Extract relevant metrics from the confusion matrix\n",
    "  accuracy <- cm$overall[\"Accuracy\"]\n",
    "  precision <- cm$byClass[\"Precision\"]\n",
    "  recall <- cm$byClass[\"Recall\"]\n",
    "  f1 <- cm$byClass[\"F1\"]\n",
    "  specificity <- cm$byClass[\"Specificity\"]\n",
    "  \n",
    "  # Return a list of metrics\n",
    "  return(list(\n",
    "    accuracy = accuracy,\n",
    "    precision = precision,\n",
    "    recall = recall,\n",
    "    f1 = f1,\n",
    "    specificity = specificity\n",
    "  ))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_lgbm(lgbm, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_lgbm(lgbm, x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix <- function(model, x_validation, y_validation) {\n",
    "    class_names <- c(\"Grassland\", \"No Grassland\")\n",
    "\n",
    "    predictions <- predict(model, as.matrix(x_validation))\n",
    "    predictions_binary <- ifelse(predictions > 0.9, 1, 0)\n",
    "\n",
    "    cm <- confusionMatrix(factor(predictions_binary), as.factor(y_validation))\n",
    "    cm_normalized <- prop.table(cm$table, 1)\n",
    "    cm_data <- as.data.frame(cm_normalized)\n",
    "    colnames(cm_data) <- c(\"True\", \"Predicted\", \"Freq\")\n",
    "\n",
    "    ggplot(cm_data, aes(y = Predicted, x = True, fill = Freq)) +\n",
    "         coord_equal() +\n",
    "         geom_tile() +\n",
    "         geom_text(aes(label = sprintf(\"%.2f\", Freq)), size = 5) +\n",
    "         scale_fill_gradient(low = \"white\", high = \"darkgreen\") +\n",
    "         scale_x_discrete(labels = class_names) +\n",
    "         scale_y_discrete(labels = class_names) +\n",
    "         labs(title = \"LGBM\", y = \"True labels\", x = \"Predicted labels\") +\n",
    "         theme_minimal() +\n",
    "         theme(\n",
    "          legend.position = \"none\",\n",
    "          axis.title = element_text(size = 14),\n",
    "          axis.text = element_text(size = 12),\n",
    "          plot.title = element_text(size = 16)\n",
    "         )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(lgbm, x_validation, y_validation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "After completing this first machine learning workflow for a limited area, we can draw several conclusions:\n",
    "\n",
    "- **The best-performing models for this case were the Random Forest and the LightGBM, both tree-based ML algorithms.** \n",
    "\n",
    "- **Our preferred choice for this case is LightGBM, primarily due to its superior speed and memory efficiency, as well as its ability to effectively handle multi-dimensional datasets.**\n",
    "\n",
    "- **Training the model with limited areas may lead to overfitting due to the correlation between adjacent pixels. In order to address this issue, our upcoming step involves scaling up the analysis and implementing measures to reduce the impact of adjacent pixel correlation.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Training Neural Network \n",
    "\n",
    "This is just a bonus showing a quick demonstration of using a neural network for this classification task. The focus of this notebook is not on neural nets, but anyway it might be an interesting starting point for further exploration of this area of machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a Sequential model\n",
    "model <- keras_model_sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "model %>%\n",
    "  layer_dense(units = 64, activation = 'relu', input_shape = ncol(X_train)) %>%\n",
    "  layer_dense(units = 64, activation = 'relu') %>%\n",
    "  layer_dense(units = 1, activation = 'sigmoid')\n",
    "\n",
    "# Compile the model\n",
    "model %>% compile(\n",
    "  loss = 'binary_crossentropy',\n",
    "  optimizer = optimizer_adam(),\n",
    "  metrics = c('accuracy')\n",
    ")\n",
    "\n",
    "# Display the model's architecture\n",
    "model %>% summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "hist <- model %>% fit(\n",
    "  x = X_train,\n",
    "  y = y_train,\n",
    "  validation_data = list(X_test, y_test),\n",
    "  epochs = 20,\n",
    "  batch_size = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the accuracy and validation accuracy from the history object\n",
    "acc <- hist$metrics$accuracy\n",
    "val_acc <- hist$metrics$val_accuracy\n",
    "epochs <- 1:length(acc)\n",
    "\n",
    "# Create a data frame for plotting\n",
    "df <- data.frame(\n",
    "  Epoch = epochs,\n",
    "  Accuracy = acc,\n",
    "  Validation_Accuracy = val_acc\n",
    ")\n",
    "\n",
    "# Plotting the training and validation accuracy\n",
    "ggplot(df, aes(x = Epoch)) +\n",
    "  geom_line(aes(y = Accuracy, color = \"Training Accuracy\"), linetype = \"solid\") +\n",
    "  geom_line(aes(y = Validation_Accuracy, color = \"Validation Accuracy\"), linetype = \"dashed\") +\n",
    "  labs(title = \"Training and Validation Accuracy\",\n",
    "       x = \"Epoch\",\n",
    "       y = \"Accuracy\") +\n",
    "  scale_color_manual(name = \"Legend\", values = c(\"Training Accuracy\" = \"blue\", \"Validation Accuracy\" = \"red\")) +\n",
    "  theme_minimal() +\n",
    "  theme(legend.position = \"bottom\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
