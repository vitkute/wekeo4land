{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../media/common/LogoWekeo_Copernicus_RGB_0.png' align='left' height='96px'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grassland Classification\n",
    "\n",
    "*Authors: Adrian Di Paolo, Chung-Xiang Hong, Jonas Viehweger* \n",
    "\n",
    "This notebook will demonstrate how to clean up and pre process satellite data for a machine learning task. It will also go into detail on how to use the cleaned data to train, evaluate and select a model for a larger scale application. \n",
    "\n",
    "The task is to train a model which can classify grassland areas in the Netherlands. The ultimate goal would be to classify grasslands yearly to derive change maps of grassland loss and gain from them. To do the classification we are using phenological data and the EuroCrops Dataset as ground truth.\n",
    "\n",
    "This notebook uses data that was already downloaded and prepared for Python in the previous notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Preprocessing Data](#preprocessing-data)\n",
    "\n",
    "    - [Labeling Data](#labeling-the-data)\n",
    "\n",
    "    - [Data Cleaning](#data-cleaning)\n",
    "\n",
    "    - [Normalizing Dates](#normalizing-dates)\n",
    "\n",
    "    - [Normalizing Numerical Data](#normalize-numerical-data)\n",
    "\n",
    "    - [Split Dataset into Train and Test](#split-the-dataset-into-train-and-test) \n",
    "\n",
    "2. [Model Training](#model-training)\n",
    "\n",
    "    \n",
    "3. [Model Evaluation](#model-evaluation)\n",
    "\n",
    "    - [Metrics](#metrics)\n",
    "\n",
    "    - [Memory Usage](#memory-usage)\n",
    "\n",
    "    - [Execution Time](#execution-time) \n",
    "\n",
    "    - [Confusion Matrix](#confusion-matrtix)\n",
    "\n",
    "    - [ROC Curve](#roc-curve)\n",
    "    \n",
    "4. [Model Selection](#model-selection)\n",
    "\n",
    "    - [Fine-tuning](#hyperparameter-optimization)\n",
    "\n",
    "    - [Feature Importance](#feature-importance)\n",
    "\n",
    "5. [Neural Network](#bonus-training-neural-network)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import load\n",
    "import pandas as pd\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we load the downloaded data\n",
    "x_data = load('../../data/processing/ml-grassland-classification/dataset/x_data.npy')\n",
    "y_data = load('../../data/processing/ml-grassland-classification/dataset/y_data.npy')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labeling the data\n",
    "\n",
    "We assign a 1 to the grassland label and 0 for others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_label(y_data):\n",
    "    \"\"\"Assigns new labels: 1 if grassland, else 0\"\"\"\n",
    "    binary_assign = np.vectorize(lambda x: 1 if x=='3302000000.0' else 0)\n",
    "    y_data = y_data.astype(str)\n",
    "    y_data = binary_assign(y_data)\n",
    "    return y_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning\n",
    "\n",
    "In our dataset we have Nan values or values that indicate No Data in the HRVPP documentation, so we delete the rows containing those values from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_df(x_data, y_data=None):\n",
    "\n",
    "    \"\"\"Transform the data to dataframe and cleans it\"\"\"\n",
    "    x_df = pd.DataFrame(x_data, columns = ['AMPL', 'EOSD', 'EOSV', 'LENGTH', 'LSLOPE', 'MAXD', 'MAXV', 'MINV', 'QFLAG', 'RSLOPE', 'SOSD', 'SOSV', 'SPROD', 'TPROD'])\n",
    "\n",
    "    if y_data is not None:\n",
    "        y_df = pd.DataFrame(y_data, columns=['LABEL'])\n",
    "        df = x_df.join(y_df)\n",
    "    else:\n",
    "        df = x_df\n",
    "    df = df.dropna()\n",
    "    df = df[~(df[['EOSD', 'SOSD', 'MAXD', 'LENGTH']] == 0).any(axis=1)]\n",
    "    df = df[~(df[['SOSV', 'EOSV', 'MAXV', 'MINV', 'AMPL', 'LSLOPE', 'RSLOPE']] == 32768).any(axis=1)]\n",
    "    df = df[~(df[['SPROD', 'TPROD']] == 65535).any(axis=1)]\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing Dates\n",
    "\n",
    "As we are dealing with datasets that include columns with dates, it is essential to ensure that those columns are normalized. \n",
    "\n",
    "In our case, we have a dataset for different years that come with dates in the format 'YYDOY'. So the first two digits representing the year, and the last three digits representing the day of the year. As we need those dates to be consistent with other years, and also be in the same range, we transform them to a number representing the count of days since a reference date (1st of January of the previous year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dates(df):\n",
    "    \"\"\"Transform dates columns from YYDOY to YY-MM-DD and then to the days since the 1 january of the previous year\"\"\"\n",
    "    \n",
    "    df[['EOSD', 'SOSD', 'MAXD']] = df[['EOSD', 'SOSD', 'MAXD']].astype(str)\n",
    "    df[['SOSD', 'EOSD', 'MAXD']] = df[['SOSD', 'EOSD', 'MAXD']].apply(lambda x: pd.to_datetime(x, format='%y%j'))\n",
    "    \n",
    "    min_year = df['SOSD'].dt.year.min()\n",
    "    reference_date = pd.Timestamp(year=min_year, month=1, day=1)\n",
    "    df['SOSD'] = (df['SOSD'] - reference_date).dt.days\n",
    "    df['EOSD'] = (df['EOSD'] - reference_date).dt.days\n",
    "    df['MAXD'] = (df['MAXD'] - reference_date).dt.days\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Normalizing Numerical Data\n",
    "\n",
    "To ensure that our machine learning models perform optimally, it is essential to rescale the numerical data. Rescaling, transforms the data to a common scale without distorting differences in the ranges of values. This process is crucial for algorithms that compute distances between data points, such as gradient boosting and neural networks, as it ensures that features with larger ranges do not dominate the learning process.  \n",
    "\n",
    "In this notebook, we will apply Min-Max scaling to bring all numerical features into the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_df(df_data):\n",
    "    scaler = MinMaxScaler()\n",
    "    v_columns_to_scale = df_data.drop(['LABEL', 'MAXD', 'SOSD', 'EOSD'], axis=1)\n",
    "    scaled_data = scaler.fit_transform(v_columns_to_scale)\n",
    "    scaled_data = pd.DataFrame(scaled_data, columns=v_columns_to_scale.columns)\n",
    "    df_data = df_data.reset_index(drop=True)\n",
    "    df_data = pd.concat([scaled_data, df_data[['LABEL', 'MAXD', 'SOSD', 'EOSD']]], axis=1)\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for the Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = binary_label(y_data)\n",
    "\n",
    "df_data = data_to_df(x_data, y_data)\n",
    "df_data = transform_dates(df_data)\n",
    "df_data = normalize_df(df_data)\n",
    "\n",
    "x_data = df_data.drop(columns=['LABEL'])\n",
    "y_data = df_data['LABEL']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split Dataset into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.20, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersampling Dataset\n",
    "\n",
    "Undersampling is a technique used to balance imbalanced datasets, where one class has significantly more samples than another class.\n",
    "\n",
    "The main advantage of undersampling is that it can improve the performance of classifiers by reducing the bias towards the majority class, which can lead to better predictions on the minority class. Undersampling can also reduce the training time and memory requirements of the model, since there are fewer instances to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class distribution before undersampling\n",
    "class_counts_before = pd.Series(y_train).value_counts()\n",
    "print(class_counts_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampler = RandomUnderSampler(sampling_strategy='majority')\n",
    "X_train, y_train = undersampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class distribution after undersampling\n",
    "class_counts_after = pd.Series(y_train).value_counts()\n",
    "\n",
    "print(class_counts_before)\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(12, 6))\n",
    "\n",
    "# Create pie chart on first subplot\n",
    "axes[0].pie(class_counts_before, labels=['No Grassland', 'Grassland'], autopct='%1.1f%%', colors=['tab:grey', 'tab:green'],\n",
    "              wedgeprops={'linewidth': 2, 'edgecolor': 'white', 'alpha': 0.7})\n",
    "axes[0].set_title('Class Distribution Before Undersampling')\n",
    "\n",
    "# Create pie chart on second subplot\n",
    "axes[1].pie(class_counts_after, labels=['No Grassland', 'Grassland'], autopct='%1.1f%%', colors=['tab:grey', 'tab:green'],\n",
    "              wedgeprops={'linewidth': 2, 'edgecolor': 'white', 'alpha': 0.7})\n",
    "axes[1].set_title('Class Distribution After Undersampling')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "#plt.subplots_adjust(wspace=0.4)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "Here we are testing quite a few different algorithms on their performance for the task. \n",
    "It has to be noted that this is a quite naive and brute force approach to the task, since the models hyperparameters aren't tweaked and no pre-selection of machine learning algorithms based on expert knowledge is made. \n",
    "\n",
    "However it will give a rough idea on the performance of the algorithms and in addition it will provide information on the computational efficiency of the algorithms in terms of memory usage and computation time. These are also important parameters to consider when scaling the model up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "# Logistic Regression\n",
    "models['Logistic Regression'] = LogisticRegression(max_iter=1000, n_jobs=4)\n",
    "\n",
    "# Support Vector Machines\n",
    "models['Support Vector Machines'] = LinearSVC(dual=False)\n",
    "\n",
    "# Decision Trees\n",
    "models['Decision Trees'] = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Random Forest\n",
    "models['Random Forest'] = RandomForestClassifier(n_jobs=4, random_state=42)\n",
    "\n",
    "# Naive Bayes\n",
    "models['Naive Bayes'] = GaussianNB()\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "models['K-Nearest Neighbor'] = KNeighborsClassifier(n_jobs=4)\n",
    "\n",
    "# LightGBM\n",
    "models['Lightgbm'] = LGBMClassifier(n_jobs=4, random_state=42)\n",
    "\n",
    "# Metrics\n",
    "accuracy, precision, recall, f1, memory_usage, time_usage = {}, {}, {}, {}, {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in models.keys():\n",
    "    \n",
    "    # Fit the classifier model\n",
    "    start_time = time.time()\n",
    "    start_mem = psutil.Process().memory_info().rss / 1024 / 1024 # in MB\n",
    "    models[key].fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    end_mem = psutil.Process().memory_info().rss / 1024 / 1024 # in MB\n",
    "    \n",
    "    # Prediction \n",
    "    predictions = models[key].predict(X_test)\n",
    "    elapsed_time = end_time - start_time\n",
    "    mem_usage = end_mem - start_mem\n",
    "    info_text = f'Time: {elapsed_time:.2f} s\\nMemory: {mem_usage:.2f} MB'\n",
    "\n",
    "    # Calculate Accuracy, Precision and Recall, F1-Score, and Memory and Usage Metrics\n",
    "    accuracy[key] = accuracy_score(predictions, y_test)\n",
    "    precision[key] = precision_score(predictions, y_test)\n",
    "    recall[key] = recall_score(predictions, y_test)\n",
    "    f1[key] = f1_score(predictions, y_test)\n",
    "    memory_usage[key] = mem_usage\n",
    "    time_usage[key] = elapsed_time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dict(data, title, x_label, y_label):\n",
    "    df = pd.DataFrame.from_dict(data, orient='index', columns=['metric'])\n",
    "\n",
    "    # Sort the DataFrame by the metric column\n",
    "    df = df.sort_values(by='metric')\n",
    "\n",
    "    # Create a color palette for the bars\n",
    "    colors = sns.color_palette(\"Greens\", len(data))\n",
    "\n",
    "    # Set the style and context using sns.set()\n",
    "    sns.set(style=\"whitegrid\", context=\"notebook\")\n",
    "    ax = sns.barplot(x='metric', y=df.index, data=df, hue=df.index, legend=False, palette=colors)\n",
    "\n",
    "    # Add a title and axis labels\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dict(f1, 'Models F1-Score', 'F1-Score', 'Model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dict(memory_usage, 'Models Memory Usage', 'Memory (MB)', 'Model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Execution Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dict(time_usage, 'Training Execution Time', 'Time (s)', 'Model')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the Lightgbm and the RandomForest are the two models that get better results, but in terms of time and memory usage the Lightgbm model is by far the more efficient."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Validation Dataset\n",
    "\n",
    "Now we will prove this model with the validation dataset, which was taken from a different bounding box area. This will give us the performance of the model on data which hasn't been seen during the training. If the performance of the model is much worse for this dataset, it means that the model has been overfit on the training data and isn't general enough to get a good performance on new data.\n",
    "\n",
    "We also have to pre-process the validation data them with the sane steps as explained before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_validation = load('../../data/processing/ml-grassland-classification/dataset/x_validation.npy')\n",
    "y_validation = load('../../data/processing/ml-grassland-classification/dataset/y_validation.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validation = binary_label(y_validation)\n",
    "\n",
    "df_validation = data_to_df(x_validation, y_validation)\n",
    "df_validation = transform_dates(df_validation)\n",
    "df_validation = normalize_df(df_validation)\n",
    "\n",
    "x_validation = df_validation.drop(columns=['LABEL'])\n",
    "y_validation = df_validation['LABEL']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating the models performance in the validation dataset, we will use confusion matrices and ROC curves. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "\n",
    "A confusion matrix is a table that summarizes the performance of a classification model by comparing the predicted labels with the true labels of a set of data. \n",
    "\n",
    "The confusion matrix consists of four values: true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN). The rows of the matrix represent the actual labels, while the columns represent the predicted labels. The diagonal elements of the matrix represent the instances that are classified correctly, while the off-diagonal elements represent the instances that are misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(models, X_test, y_test):\n",
    "    num_models = len(models)\n",
    "    nrows = (num_models + 2) // 3\n",
    "    ncols = min(num_models, 3)\n",
    "    fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 5*nrows))\n",
    "    class_names = ['Grassland', 'No Grassland']\n",
    "    for i, model in enumerate(models):\n",
    "        y_pred = model.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred, normalize='true')  # normalize the confusion matrix\n",
    "        \n",
    "        row = i // ncols\n",
    "        col = i % ncols\n",
    "        ax = axs[row][col]\n",
    "        sns.heatmap(cm, annot=True, cmap='Greens', fmt='.2f', xticklabels=class_names, yticklabels=class_names, cbar=False, ax=ax)\n",
    "        ax.set_xlabel('Predicted labels', fontsize=14)\n",
    "        ax.set_ylabel('True labels', fontsize=14)\n",
    "        ax.set_title(type(model).__name__, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    if num_models % 3 != 0:\n",
    "        for i in range(num_models % 3, 3):\n",
    "            axs[-1][i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrices(list(models.values()), x_validation, y_validation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve\n",
    "\n",
    "The ROC (Receiver Operating Characteristic) curve is a graphical representation of the performance of a binary classifier. The ROC curve shows the trade-off between the true positive rate (TPR), also called sensitivity or recall, and the false positive rate (FPR), which is the proportion of negative instances that are incorrectly classified as positive.\n",
    "\n",
    "To create a ROC curve, the classifier's output is sorted by confidence or probability, and the threshold for classification is varied from high to low. At each threshold value, the TPR and FPR are calculated and plotted on a graph with TPR on the y-axis and FPR on the x-axis. The resulting curve represents the classifier's performance at all possible threshold values.\n",
    "\n",
    "The closer the curve is to the top-left corner of the graph, the better the classifier's performance, as this indicates a high TPR and a low FPR. The area under the ROC curve (AUC) is a commonly used metric to summarize the classifier's performance. A perfect classifier would have an AUC of 1, while a random classifier would have an AUC of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(models, X_test, y_test, labels, figsize=(8, 6)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    for model, label in zip(models, labels):\n",
    "        if label != 'Support Vector Machines':\n",
    "            y_prob = model.predict_proba(X_test)[:, 1]\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f'{label} (AUC = {roc_auc:.2f})')\n",
    "        else:\n",
    "            y_score = model.decision_function(X_test)\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f'{label} (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(list(models.values()), x_validation, y_validation, list(models.keys()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "\n",
    "As we can see for the previous metrics, memory and time usage, the best candidate to solve this binary classification problem is the LightGBM model.\n",
    "\n",
    "LightGBM is a popular open-source gradient boosting framework that was developed by Microsoft. It is designed to be highly efficient in terms of training speed and memory usage, making it a popular choice for large-scale machine learning tasks. LightGBM uses gradient boosting algorithms to build models, which iteratively improves the performance of a weak learner by adding new decision trees to the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMClassifier(n_jobs=4, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter Optimization\n",
    "\n",
    "GridSearchCV is a technique used to fine-tune the hyperparameters in order to improve its performance. In essence, it involves searching over a range of values for each hyperparameter and finding the combination that yields the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary of hyperparameters to search over\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01],\n",
    "    'num_leaves': [200, 300],\n",
    "    'max_depth': [7, 10, 14]\n",
    "}\n",
    "\n",
    "# Define a grid search object with the LGBM classifier and hyperparameters to search over\n",
    "grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, cv=5, scoring='f1')\n",
    "\n",
    "# Fit the grid search object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and their corresponding score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best f1 score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMClassifier(**grid_search.best_params_, n_jobs=4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm.score(x_validation, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lgbm.predict(x_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_validation, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Importance\n",
    "One way to gain insight into the relative importance of features is to use the feature_importances_ attribute. This is useful to understand which features were most important in predicting the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = dict(zip([\"AMPL\",\"EOSD\",\"EOSV\",\"LENGTH\",\"LSLOPE\",\"MAXD\",\"MAXV\",\"MINV\",\"QFLAG\",\"RSLOPE\",\"SOSD\",\"SOSV\",\"SPROD\",\"TPROD\"], lgbm.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dict(feature_importances, 'Feature Importance', 'Importance', 'Feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_validation, y_pred):\n",
    "    class_names = ['GrassLand', 'No Grassland']\n",
    "    cm = confusion_matrix(y_validation, y_pred, labels=[1, 0], normalize='true')\n",
    "    sns.heatmap(cm, annot=True, xticklabels=class_names, yticklabels=class_names, cmap='Greens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_validation, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "After completing this first machine learning workflow for a limited area, we can draw several conclusions:\n",
    "\n",
    "- **The best-performing models for this case were the Random Forest and the LightGBM, both tree-based ML algorithms.** \n",
    "\n",
    "- **Our preferred choice for this case is LightGBM, primarily due to its superior speed and memory efficiency, as well as its ability to effectively handle multi-dimensional datasets.**\n",
    "\n",
    "- **Training the model with limited areas may lead to overfitting due to the correlation between adjacent pixels. In order to address this issue, our upcoming step involves scaling up the analysis and implementing measures to reduce the impact of adjacent pixel correlation.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Training Neural Network \n",
    "\n",
    "This is just a bonus showing a quick demonstration of using a neural network for this classification task. The focus of this notebook is not on neural nets, but anyway it might be an interesting starting point for further exploration of this area of machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Dense(64, activation='relu', input_dim=14))\n",
    "model.add(Dense(64, activation='relu', input_dim=14))\n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "acc = hist.history['accuracy']\n",
    "val = hist.history['val_accuracy']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, '-', label='Training accuracy')\n",
    "plt.plot(epochs, val, ':', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
