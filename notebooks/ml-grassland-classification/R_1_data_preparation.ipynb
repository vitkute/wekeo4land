{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../media/common/LogoWekeo_Copernicus_RGB_0.png' align='left' height='96px'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input data for training a supervised learning model\n",
    "\n",
    "*Authors: Adrian Di Paolo, Chung-Xiang Hong, Jonas Viehweger* \n",
    "\n",
    "This notebook shows the steps towards preparing data for training a supervised machine learning model. We will train the model based on the [pan-European High Resolution Vegetation Phenology and Productiviy (HR-VPP)](https://collections.sentinel-hub.com/vegetation-phenology-and-productivity-parameters-season-1/) data, which is derived from ESA’s Sentinel-2 as part of the Copernicus Land Monitoring Service (CLMS). The 13 parameters that describe specific stages of the seasonal vegetation growth cycle will be used as input features to fit a model.\n",
    "\n",
    "As for the ground truth, we will use the [EuroCrops](https://github.com/maja601/EuroCrops#vectordata_zenodo) data, which is a dataset combining all publicly available self-declared crop reporting datasets from countries of the European Union.\n",
    "\n",
    "In this example notebook, the expected outcome is a binary classifier that identifies grassland and non-grassland areas in the Netherlands. We will start by preparing a small dataset to train models with different algorithms, so we can have an overview on the performance of the models that is essential in the following model selection process.\n",
    "\n",
    "Preparing the data at small scale for training a model can be simple and straightforward with the following three steps:\n",
    "1. [Define training and validation area](#1-define-training-and-validation-area)\n",
    "2. [Obtain features and labels](#2-obtain-features-and-labels)\n",
    "3. [Reshape data for model training](#3-reshape-data-for-model-training)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "\n",
    "library(fs)\n",
    "library(dplyr)\n",
    "\n",
    "library(sf)\n",
    "library(terra)\n",
    "library(stars)\n",
    "library(purrr)\n",
    "\n",
    "library(leaflet)\n",
    "library(ggplot2)\n",
    "\n",
    "library(hdar)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define training and validation area\n",
    "\n",
    "To train a machine learning model, we need to define a training dataset and a validation dataset. The training data set is a set of example data which is used to fit the parameters (e.g., weights) during the learning process. Then, the trained model needs to be evaluated using examples from the held-out dataset, which is the validation dataset that is independent of the training dataset and is not used in the training process.\n",
    "\n",
    "In our case, we will randomly select a training area and a validation area in the Netherlands to obtain examples for the training and the evaluation process. First let's take a look at these two areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# define bounding box\n",
    "bounds_poly <- st_read(\"../../data/raw/ml-grassland-classification/bounding_boxes.geojson\")\n",
    "validation_gdf <- bounds_poly[nrow(bounds_poly), ]\n",
    "train_gdf <- bounds_poly[-nrow(bounds_poly), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Transform the data to WGS84 (EPSG:4326)\n",
    "total_bbox <- st_transform(bounds_poly, crs = 4326) %>% st_bbox()\n",
    "validation_bbox <- st_transform(validation_gdf, crs = 4326) %>% st_bbox()\n",
    "train_bbox <- st_transform(train_gdf, crs = 4326) %>% st_bbox()\n",
    "\n",
    "print(train_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create the map\n",
    "map <- leaflet() %>%\n",
    "  addTiles() %>%\n",
    "  setView(lng = 7.014084, lat = 53.104538500000004, zoom = 8)\n",
    "\n",
    "# Add validation geometries in red\n",
    "map <- map %>%\n",
    "  addPolygons(data = st_transform(validation_gdf, crs = 4326), color = \"red\", weight = 2)\n",
    "\n",
    "# Add training geometries in blue\n",
    "map <- map %>%\n",
    "  addPolygons(data = st_transform(train_gdf, crs = 4326), color = \"blue\", weight = 2)\n",
    "\n",
    "map\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the ground truth dataset\n",
    "\n",
    "Since we are going to train a binary classification model to identify if an area is grassland or not in the Netherlands, it is important to investigate the distribution of crop types in the country.  \n",
    "\n",
    "Here we display the distribution of the top 10 categories reported in the Netherlands. We can see that grassland is the most dominant category in the data, which is a critical point we should pay attention to when sampling data for training at a national scale.\n",
    "\n",
    "**Note** that [EuroCrops Netherlands](https://zenodo.org/record/7476474/files/NL_2020.zip?download=1) data needs to be downloaded and saved in the same folder where this notebook is located.\n",
    "\n",
    "This is also quite a big dataset, so loading it entirely may take a while."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "curl -L \"https://zenodo.org/record/7476474/files/NL_2020.zip?download=1\" -o ../../data/download/ml-grassland-classification/NL_2020.zip\n",
    "unzip -o ../../data/download/ml-grassland-classification/NL_2020.zip -d ../../data/processing/ml-grassland-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# load full dataset to GeoDataFrame\n",
    "EC_NETHERLANDS_PATH <- \"../../data/processing/ml-grassland-classification/NL/NL_2020_EC21.shp\"\n",
    "netherlands_gt <- st_read(EC_NETHERLANDS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "knitr::kable(head(netherlands_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "categories <- netherlands_gt %>%\n",
    "              st_drop_geometry %>%\n",
    "              count(EC_hcat_n, sort = TRUE)\n",
    "\n",
    "# create a subset for the top 10 catagories\n",
    "categories_subset <- categories %>% slice_head(n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Adjust the plot size\n",
    "options(repr.plot.width = 24, repr.plot.height = 12)\n",
    "\n",
    "ggplot(categories_subset, aes(x = \"\", y = n, fill = EC_hcat_n)) +\n",
    "  geom_bar(width = 1, stat = \"identity\") +\n",
    "  coord_polar(theta = \"y\") +\n",
    "  theme_void() +\n",
    "  theme(legend.position = \"right\", \n",
    "      legend.text = element_text(size = 20),\n",
    "      legend.title = element_text(size = 24),\n",
    "      legend.margin = margin(0, 24, 0, 0),\n",
    "  ) +\n",
    "  geom_text(aes(label = scales::percent(n / sum(n), accuracy = 0.1)),\n",
    "    position = position_stack(vjust = 0.5), size = 5,\n",
    "  ) +\n",
    "  labs(fill = \"Categories\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's have a closer look at the data in the training area and the validation area. We can see the distribution of crop types is much more balanced in both the training area and validation area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "train_gt <- st_read(EC_NETHERLANDS_PATH, wkt_filter = st_as_text(st_as_sfc(train_bbox)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Adjust the plot size\n",
    "options(repr.plot.width=14, repr.plot.height=12)\n",
    "\n",
    "# plot all geometries in the training area\n",
    "ggplot() +\n",
    "  geom_sf(data = train_gt) +\n",
    "  theme_minimal() +\n",
    "  theme(axis.text = element_text(size = 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# create pie chart\n",
    "top_categories <- train_gt %>%\n",
    "  st_drop_geometry() %>% # Remove geometry for counting\n",
    "  count(EC_hcat_n, sort = TRUE) %>%\n",
    "  slice_head(n = 10)\n",
    "\n",
    "options(repr.plot.width = 20, repr.plot.height = 14)\n",
    "\n",
    "ggplot(top_categories, aes(x = \"\", y = n, fill = EC_hcat_n)) +\n",
    "  geom_bar(width = 1, stat = \"identity\") +\n",
    "  coord_polar(\"y\") +\n",
    "  theme_void() +\n",
    "  theme(legend.position = \"right\", \n",
    "        legend.text = element_text(size = 20),\n",
    "        legend.title = element_text(size = 24),\n",
    "        legend.margin = margin(0, 24, 0, 0)\n",
    "  ) +\n",
    "  geom_text(aes(label = scales::percent(n / sum(n), accuracy = 0.1)),\n",
    "    position = position_stack(vjust = 0.5), size = 5\n",
    "  ) +\n",
    "  labs(fill = \"Top Categories\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "validation_gt <- st_read(EC_NETHERLANDS_PATH, wkt_filter = st_as_text(st_as_sfc(validation_bbox)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Adjust the plot size\n",
    "options(repr.plot.width=14, repr.plot.height=12)\n",
    "\n",
    "# plot all geometries in the training area\n",
    "ggplot() +\n",
    "  geom_sf(data = validation_gt) +\n",
    "  theme_minimal() +\n",
    "  theme(\n",
    "    plot.title = element_text(size = 20),\n",
    "    axis.title = element_text(size = 16),\n",
    "    axis.text = element_text(size = 14)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Count unique elements in EC_hcat_n and get the top 10\n",
    "top_categories_validation <- validation_gt %>%\n",
    "  st_drop_geometry() %>%  # Remove geometry for counting\n",
    "  count(EC_hcat_n, sort = TRUE) %>%\n",
    "  slice_head(n = 10)\n",
    "\n",
    "options(repr.plot.width = 20, repr.plot.height = 14)\n",
    "\n",
    "# Create the pie chart\n",
    "ggplot(top_categories_validation, aes(x = \"\", y = n, fill = EC_hcat_n)) +\n",
    "  geom_bar(width = 1, stat = \"identity\") +\n",
    "  coord_polar(theta = \"y\") +\n",
    "  theme_void() +\n",
    "  theme(legend.position = \"right\", \n",
    "        legend.text = element_text(size = 16),\n",
    "        legend.title = element_text(size = 20),\n",
    "        legend.margin = margin(0, 24, 0, 0)\n",
    "  ) +\n",
    "  geom_text(aes(label = scales::percent(n / sum(n), accuracy = 0.1)),\n",
    "    position = position_stack(vjust = 0.5), size = 5\n",
    "  ) +\n",
    "  labs(fill = \"Categories\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias\n",
    "\n",
    "Training and validating a model with samples in small areas will introduce a bias. The bias comes from two aspects:\n",
    "* The imbalanced samples of grassland and non-grassland\n",
    "* The spatial correlation between samples\n",
    "\n",
    "This will be addressed later when preparing data at a large scale to train a model general enough for the whole country."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Obtain HR-VPP data and labels\n",
    "\n",
    "In total we will show two possibilities to get HR-VPP data. The first is by using the HDA API which is available on WEkEO. This API has a few limitations which necessitate a bit more post-processing of the data. In a later notebook we will show how to access the data using the Sentinelhub API which is more capable in its processing.\n",
    "\n",
    "### Downloading Data\n",
    "\n",
    "First the data is downloaded from WEkEO using the HDA R package. A getting started tutorial for the API is available [here](https://help.wekeo.eu/en/articles/6751608-what-is-the-hda-api-python-client-and-how-to-use-it). \n",
    "\n",
    "For the download we specify a helper function which subsets the data query to only a single tile. This is done to reduce the amount of data that will be downloaded, since a lot of tiles are overlapping the study area. The HDA API will always download entire tiles even if you are only interested in small parts of the tile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "hda_tile_download <- function(client, query, tile, folder) {\n",
    "  matches <- client$search(query)\n",
    "\n",
    "  matches_subset <- Filter(function(match) grepl(tile, match$properties$location), matches$results)\n",
    "  matches$results <- matches_subset\n",
    "\n",
    "  matches$download(folder, prompt = FALSE)\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the query we specify which dataset we want to download, for which timeframe and for which area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HDA API downloads different HR-VPP parameters in separate TIFF files. So the next step is to read in the data for our training and validation areas and stack them together. To ensure you download all the necessary parameters, remember to adjust the “productType” with the specific parameter IDs: `'AMPL', 'EOSD', 'EOSV', 'LENGTH', 'LSLOPE', 'MAXD', 'MAXV', 'MINV', 'RSLOPE', 'SOSD', 'SOSV', 'SPROD', 'TPROD'`. This will ensure the correct set of HR-VPP data is included for your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "c <- Client$new()\n",
    "\n",
    "product_types <- list('AMPL', 'EOSD', 'EOSV', 'LENGTH', 'LSLOPE', 'MAXD', 'MAXV', 'MINV', 'QFLAG', 'RSLOPE', 'SOSD', 'SOSV', 'SPROD', 'TPROD')\n",
    "\n",
    "for (product_type in product_types) {\n",
    "\n",
    "  query_obj <- list(\n",
    "    dataset_id = \"EO:EEA:DAT:CLMS_HRVPP_VPP\",\n",
    "    productType = product_type,\n",
    "    productVersion = \"V101\",\n",
    "    productGroupId = \"s1\",\n",
    "    start = \"2020-01-01T00:00:00.000Z\",\n",
    "    end = \"2020-01-01T00:00:00.000Z\",\n",
    "    bbox = as.numeric(total_bbox)\n",
    "  )\n",
    "\n",
    "  query <- jsonlite::toJSON(query_obj, auto_unbox = TRUE)\n",
    "  hda_tile_download(c, query, tile = \"T32ULD\", folder = \"../../data/download/ml-grassland-classification/hda\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dir_path <- \"../../data/download/ml-grassland-classification/hda/\"\n",
    "file_list <- list.files(path = dir_path, pattern = \"\\\\.tif$\", full.names = TRUE)\n",
    "file_list <- sort(file_list)\n",
    "\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "read_raw_raster <- function(file_path) {\n",
    "\n",
    "  float_values <- rast(file_path)\n",
    "\n",
    "  # Get the scale factor and offset using scoff\n",
    "  scale_offset <- scoff(float_values)\n",
    "\n",
    "  # Initialize scale and offset\n",
    "  scale_factor <- scale_offset[[1]]\n",
    "  offset <- scale_offset[[2]]\n",
    "\n",
    "  # Adjust the values based on scale and offset if present\n",
    "  raw_values <- (float_values - offset) / scale_factor\n",
    "\n",
    "  round(raw_values)\n",
    "}\n",
    "\n",
    "get_affine_transform <- function(src) {\n",
    "  # Extract extent and resolution\n",
    "  ext <- ext(src)\n",
    "  res <- res(src)\n",
    "\n",
    "  # Calculate affine transformation components\n",
    "  xmin <- ext[1]\n",
    "  ymax <- ext[4]\n",
    "  xres <- res[1]\n",
    "  yres <- res[2]\n",
    "\n",
    "  # Create the affine transformation matrix\n",
    "  matrix(c(xres, 0, xmin, 0, -yres, ymax, 0, 0, 1), nrow = 3, byrow = TRUE)\n",
    "}\n",
    "\n",
    "compute_window <- function(transform, bbox) {\n",
    "  # Extract affine transform components\n",
    "  xres <- transform[1, 1]\n",
    "  yres <- abs(transform[2, 2]) # Ensure positive resolution for calculation\n",
    "  xmin <- transform[1, 3]\n",
    "  ymax <- transform[2, 3]\n",
    "\n",
    "  # Calculate pixel coordinates for the bounding box\n",
    "  col_min <- floor((bbox[\"xmin\"] - xmin) / xres)\n",
    "  col_max <- floor((bbox[\"xmax\"] - xmin) / xres)\n",
    "  row_min <- floor((ymax - bbox[\"ymax\"]) / yres)\n",
    "  row_max <- floor((ymax - bbox[\"ymin\"]) / yres)\n",
    "\n",
    "  # Create window as list of coordinates\n",
    "  window <- list(col_min = col_min, col_max = col_max, row_min = row_min, row_max = row_max)\n",
    "}\n",
    "\n",
    "crop_raster_with_bbox <- function(src, bbox) {\n",
    "  # Extract extent and resolution\n",
    "  ext <- ext(src)\n",
    "  res <- res(src)\n",
    "\n",
    "  # Calculate affine transformation components\n",
    "  xmin <- ext[1]\n",
    "  ymax <- ext[4]\n",
    "  xres <- res[1]\n",
    "  yres <- res[2]\n",
    "\n",
    "  # Calculate pixel coordinates for the bounding box\n",
    "  col_min <- floor((bbox[\"xmin\"] - xmin) / xres)\n",
    "  col_max <- floor((bbox[\"xmax\"] - xmin) / xres)\n",
    "  row_min <- floor((ymax - bbox[\"ymax\"]) / yres)\n",
    "  row_max <- floor((ymax - bbox[\"ymin\"]) / yres)\n",
    "\n",
    "  # Calculate spatial coordinates for the window extent\n",
    "  win_extent <- ext(c(xmin + col_min * xres, xmin + col_max * xres, ymax - row_max * yres, ymax - row_min * yres - 1))\n",
    "\n",
    "  # Crop the raster using the window extent\n",
    "  crop(src, win_extent, snap = \"out\")\n",
    "}\n",
    "\n",
    "compute_new_transform <- function(bounds, window) {\n",
    "  width <- window$col_max - window$col_min\n",
    "  height <- window$row_max - window$row_min\n",
    "\n",
    "  # Extract bounding box components\n",
    "  xmin <- bounds[\"xmin\"]\n",
    "  ymin <- bounds[\"ymin\"]\n",
    "  xmax <- bounds[\"xmax\"]\n",
    "  ymax <- bounds[\"ymax\"]\n",
    "\n",
    "  # Calculate the resolution\n",
    "  xres <- ceiling((xmax - xmin) / width)\n",
    "  yres <- ceiling((ymax - ymin) / height)\n",
    "\n",
    "  # Create the homogeneous affine transformation matrix (3x3)\n",
    "  matrix(c(xres, 0, xmin, 0, -yres, ymax, 0, 0, 1), nrow = 3, byrow = TRUE)\n",
    "}\n",
    "\n",
    "compute_new_bounds <- function(transform, window) {\n",
    "  # Extract affine transform components\n",
    "  xres <- transform[1, 1]\n",
    "  yres <- abs(transform[2, 2])\n",
    "  xmin <- transform[1, 3]\n",
    "  ymax <- transform[2, 3]\n",
    "\n",
    "  # Extract window parameters\n",
    "  col_min <- window$col_min\n",
    "  col_max <- window$col_max\n",
    "  row_min <- window$row_min\n",
    "  row_max <- window$row_max\n",
    "\n",
    "  # Convert window pixel coordinates to geographic coordinates\n",
    "  x_min <- xmin + col_min * xres\n",
    "  x_max <- xmin + col_max * xres\n",
    "  y_max <- ymax - row_min * yres\n",
    "  y_min <- ymax - row_max * yres\n",
    "\n",
    "  # Create the new bounding box\n",
    "  c(x_min, y_min, x_max, y_max)\n",
    "}\n",
    "\n",
    "stack_hrvpp <- function(file_list, gdf) {\n",
    "  arrays <- list()\n",
    "  new_transforms <- list()\n",
    "  bounds <- st_bbox(gdf)\n",
    "\n",
    "  for (file in file_list) {\n",
    "\n",
    "    src <- read_raw_raster(file)\n",
    "    transform <- get_affine_transform(src)\n",
    "    window <- compute_window(transform, bounds)\n",
    "\n",
    "    cropped_raster <- crop_raster_with_bbox(src, bounds)\n",
    "\n",
    "    arrays[[length(arrays) + 1]] <- as.matrix(cropped_raster, wide = TRUE)\n",
    "\n",
    "    new_bounds <- compute_new_bounds(transform, window)\n",
    "    new_transform <- compute_new_transform(new_bounds, window)\n",
    "    new_transforms[[length(new_transforms) + 1]] <- new_transform\n",
    "\n",
    "    # Remove temporary files created by terra\n",
    "    terra::tmpFiles(remove = TRUE)\n",
    "  }\n",
    "\n",
    "  stacked_array <- abind::abind(arrays, along = 3)\n",
    "\n",
    "  return(list(stacked_array, new_transforms[[1]]))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "train_result <- stack_hrvpp(file_list, train_gdf)\n",
    "train_data <- train_result[[1]]\n",
    "train_transform <- train_result[[2]]\n",
    "\n",
    "validation_result <- stack_hrvpp(file_list, validation_gdf)\n",
    "validation_data <- validation_result[[1]]\n",
    "validation_transform <- validation_result[[2]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have two arrays, one for the training area and one for the validation area, which is smaller than the one for the training area. But both arrays now have all 14 bands as their third dimension."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get labels\n",
    "\n",
    "We have the HR-VPP data of both training and validation areas now. Next, we want to get the labels for these two areas as the ground truth to perform a supervised training.\n",
    "\n",
    "To do this, we have to convert the vector data to raster data. This is done with the `get_labels` function which sets up the data for the `rasterio` function `rasterize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "get_crs <- function(tiff_path) {\n",
    "  src <- rast(tiff_path)\n",
    "  crs <- crs(src, describe=TRUE, proj=TRUE)\n",
    "  return(paste0(crs$authority, \":\", crs$code))\n",
    "}\n",
    "\n",
    "crs <- get_crs(file_list[[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "get_labels <- function(ec_gt, geotransform, out_shape, raster_crs) {\n",
    "\n",
    "  # Convert the input CRS to the raster CRS\n",
    "  ec_gt <- st_transform(ec_gt, crs = raster_crs)\n",
    "  \n",
    "  # Convert the EC_hcat_c column to numeric\n",
    "  ec_gt$label <- as.numeric(ec_gt$EC_hcat_c)\n",
    "\n",
    "  ec_gt <- ec_gt %>% select(geometry, label)\n",
    "  \n",
    "  # Extract the geotransform parameters\n",
    "  x_min <- geotransform[1, 3]\n",
    "  y_max <- geotransform[2, 3]\n",
    "  x_res <- geotransform[1, 1]\n",
    "  y_res <- -geotransform[2, 2]\n",
    "  \n",
    "  # Define the dimensions\n",
    "  nrows <- out_shape[1]\n",
    "  ncols <- out_shape[2]\n",
    "\n",
    "  # Calculate the extent of the raster\n",
    "  x_max <- x_min + ncols * x_res\n",
    "  y_min <- y_max - nrows * y_res\n",
    "  \n",
    "  # Define the extent and resolution\n",
    "  extent <- st_bbox(c(xmin = x_min, ymin = y_min, xmax = x_max, ymax = y_max), crs = raster_crs)\n",
    "\n",
    "  #raster_array <- array(-1, dim = c(ncols, nrows))\n",
    "\n",
    "  raster_template <- st_as_stars(extent, nx = ncols, ny = nrows)\n",
    "  raster_template[[1]] <- array(-1, dim = c(ncols, nrows))\n",
    "  \n",
    "  # Rasterize the vector data\n",
    "  rasterized <- st_rasterize(ec_gt, template = raster_template, values = \"label\", dx = x_res, dy = y_res)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "train_labels <- get_labels(train_gt, train_transform, dim(train_data)[1:2], crs)\n",
    "train_labels <- as.matrix(train_labels[[1]])\n",
    "\n",
    "validation_labels <- get_labels(validation_gt, validation_transform, dim(validation_data)[1:2], crs)\n",
    "validation_labels <- as.matrix(validation_labels[[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot_raster_with_legend <- function(rast, title) {\n",
    "  nrows <- nrow(rast)\n",
    "  ncols <- ncol(rast)\n",
    "\n",
    "  # Create a data frame with x, y, and value columns\n",
    "  train_labels_df <- data.frame(\n",
    "    y = rep(1:ncols, each = nrows),\n",
    "    x = rep(1:nrows, ncols),\n",
    "    value = as.vector(rast)\n",
    "  )\n",
    "\n",
    "  # Categorize the values\n",
    "  train_labels_df$category <- ifelse(train_labels_df$value > 0, \"Grassland\", \"Non-Grassland\")\n",
    "\n",
    "  # Plot the matrix using ggplot2\n",
    "  ggplot(train_labels_df, aes(x = x, y = y, fill = category)) +\n",
    "    geom_tile() +\n",
    "    scale_fill_manual(values = c(\"Grassland\" = \"#308c30\", \"Non-Grassland\" = \"#adaaaa\")) + \n",
    "    coord_equal() +\n",
    "    scale_y_reverse() +\n",
    "    theme_minimal() +\n",
    "    theme(\n",
    "      plot.title = element_text(size = 20),\n",
    "      axis.title = element_text(size = 16),\n",
    "      axis.text = element_text(size = 14),\n",
    "      legend.text = element_text(size = 20),\n",
    "      legend.title = element_text(size = 24),\n",
    "      legend.margin = margin(0, 24, 0, 0)\n",
    "    ) +\n",
    "    labs(title = title, x = \"Column Index\", y = \"Row Index\", fill = \"Value\")\n",
    "}\n",
    "\n",
    "plot_raster_with_legend(train_labels, \"Raster Plot with Legend for Training Data\")\n",
    "plot_raster_with_legend(validation_labels, \"Raster Plot with Legend for Validation Data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reshape data for model training\n",
    "\n",
    "At this point we have all the data we need to train a supervised machine learning model. The last step is to reshape the data so that it can be fed into the model. In general, we simply flatten the arrays to go from 3 dimensional data to 2 dimensional data, where the first dimension are the pixels and the second dimension the values of the bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "get_model_input <- function(features, labels) {\n",
    "\n",
    "  # Reshape the features array\n",
    "  x <- array(aperm(features, c(2, 1, 3)), dim = c(dim(features)[1] * dim(features)[2], dim(features)[3]))\n",
    "  \n",
    "  # Flatten the labels array\n",
    "  y <- as.vector(labels)\n",
    "  \n",
    "  # Filter out rows where the corresponding label is -1\n",
    "  x_clean <- x[y != -1, , drop = FALSE]\n",
    "  y_clean <- y[y != -1]\n",
    "  \n",
    "  list(x_clean = x_clean, y_clean = y_clean)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "train_model_input <- get_model_input(train_data, train_labels)\n",
    "x_data <- train_model_input$x_clean\n",
    "y_data <- train_model_input$y_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "validation_model_input <- get_model_input(validation_data, validation_labels)\n",
    "x_validation <- validation_model_input$x_clean\n",
    "y_validation <- validation_model_input$y_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# List of datasets\n",
    "datasets <- list(\n",
    "  x_validation = x_validation,\n",
    "  y_validation = y_validation,\n",
    "  x_data = x_data,\n",
    "  y_data = y_data\n",
    ")\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "dir_path <- \"../../data/processing/ml-grassland-classification/dataset\"\n",
    "dir_create(dir_path)\n",
    "\n",
    "# Save each dataset to .rds files\n",
    "for (name in names(datasets)) {\n",
    "  dataset <- datasets[[name]]\n",
    "  saveRDS(dataset, file.path(dir_path, paste0(name, \".rds\")))\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When preparing the input data for a supervised learning task, there are a few important points which can be the takeaways from this notebook:  \n",
    "1. Ground truth data is critical for a supervised learning task. It determines how well a model can learn from your data and the reliability of the validation result.\n",
    "2. It is important to have two separate dataset. One for training and the other for validation. Always make sure that the validation dataset is held-out in the training process.\n",
    "3. Training with a limited amount of samples can lead to bias. The bias can be alleviated if there are sufficient amounts of samples.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
