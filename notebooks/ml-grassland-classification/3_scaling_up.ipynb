{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../media/common/LogoWekeo_Copernicus_RGB_0.png' align='left' height='96px'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Scaling up with `eo-learn`\n",
    "\n",
    "*Authors: Adrian Di Paolo, Chung-Xiang Hong, Jonas Viehweger* \n",
    "\n",
    "`eo-learn` is a library which is especially written to prepare satellite data for machine learning workflows at large scale.\n",
    "\n",
    "In this workflow we are scaling up the previous analysis to country level. This poses new challenges but also offers benefits in terms of the generalization of the model. The challenges will be the handling of all the data, which in this case is done with `eo-learn`.\n",
    "\n",
    "To get the data in this notebook, Sentinel Hub Services are used. Those services are not free to use, however they offer a lot of benefits in terms of ease of use.\n",
    "To test this notebook anyway you can start a [free trial](https://www.sentinel-hub.com/trial) at Sentinel Hub and use this account to run the notebook. \n",
    "\n",
    "In the future this notebook can also be run using [CDSE](https://dataspace.copernicus.eu/), which will offer very similar services to Sentinel Hub to its Users for free."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Define the Area-of-Interest (AOI):](#define)\n",
    "\n",
    "2. [Accessing Data](#preprocessing-data)\n",
    "\n",
    "3. [Sampling Data](#model-training)\n",
    "\n",
    "3. [Data Preparation](#model-training)\n",
    "    \n",
    "4. [Model Training](#model-evaluation)\n",
    "\n",
    "3. [Inference & Validation](#model-training)\n",
    "\n",
    "First, some necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, some necessary imports\n",
    "\n",
    "# Jupyter notebook related\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "\n",
    "# Built-in modules\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Basics of Python data handling and visualization\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import joblib\n",
    "\n",
    "# Machine learning\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon\n",
    "from sklearn import metrics, preprocessing\n",
    "from tqdm.auto import tqdm\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sentinelhub import DataCollection, BBoxSplitter, SHConfig\n",
    "\n",
    "# Imports from eo-learn and sentinelhub-py\n",
    "from eolearn.core import (\n",
    "    EOExecutor,\n",
    "    EOPatch,\n",
    "    EOTask,\n",
    "    EOWorkflow,\n",
    "    FeatureType,\n",
    "    LoadTask,\n",
    "    OverwritePermission,\n",
    "    SaveTask,\n",
    "    linearly_connect_tasks,\n",
    ")\n",
    "from eolearn.geometry import ErosionTask, VectorToRasterTask\n",
    "from eolearn.io import ExportToTiffTask, SentinelHubEvalscriptTask, VectorImportTask\n",
    "from eolearn.ml_tools import FractionSamplingTask\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the client ID and secret from the sentinelhub account need to be defined. See [here](https://docs.sentinel-hub.com/api/latest/api/overview/authentication/) on how to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credentials import CLIENT_ID, CLIENT_SECRET\n",
    "\n",
    "config = SHConfig()\n",
    "\n",
    "config.sh_base_url = 'https://creodias.sentinel-hub.com'\n",
    "config.sh_client_secret = CLIENT_SECRET \n",
    "config.sh_client_id = CLIENT_ID\n",
    "hrvpp = DataCollection.define_byoc(\"67c73156-095d-4f53-8a09-9ddf3848fbb6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder where data for running the notebook is stored\n",
    "DATA_FOLDER = Path(\"../../data/raw\", \"ml-grassland-classification\")\n",
    "# Locations for collected data and intermediate results\n",
    "EOPATCH_FOLDER = Path(\"../../data/processing/ml-grassland-classification\", \"eopatches\")\n",
    "EOPATCH_SAMPLES_FOLDER = Path(\"../../data/processing/ml-grassland-classification\", \"eopatches_sampled\")\n",
    "RESULTS_FOLDER = Path(\"../../data/processing/ml-grassland-classification\", \"results\")\n",
    "for folder in (EOPATCH_FOLDER, EOPATCH_SAMPLES_FOLDER, RESULTS_FOLDER):\n",
    "    folder.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Define the Area-of-Interest (AOI):\n",
    "\n",
    "We are defining all of Netherlands as the Area of Interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load geojson file\n",
    "country = gpd.read_file(DATA_FOLDER / \"NL_BORDER.geojson\").to_crs(3035)\n",
    "# Add 500m buffer to secure sufficient data near border\n",
    "country = country.buffer(500)\n",
    "\n",
    "# Get the country's shape in polygon format\n",
    "country_shape = country.geometry.values[0]\n",
    "\n",
    "# Plot country\n",
    "country.plot()\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Print size\n",
    "country_width = country_shape.bounds[2] - country_shape.bounds[0]\n",
    "country_height = country_shape.bounds[3] - country_shape.bounds[1]\n",
    "print(f\"Dimension of the area is {country_width:.0f} x {country_height:.0f} m2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation of EOPatches\n",
    "\n",
    "EOPatches are the data structure which allows eo-learn to perform the data preparation at a large scale.\n",
    "They hold all types of geospatial data for a specific bounding box. The workflow which will be defined is then applied to all EOPatches individually which makes operations at large scale possible.\n",
    "\n",
    "In our case we are splitting up Netherlands into many small bounding boxes for which data is later being downloaded. \n",
    "To keep the whole workflow lighter for now we are only selecting 100 random bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a splitter to obtain a list of bboxes with 5km sides\n",
    "bbox_splitter = BBoxSplitter([country_shape], country.crs, split_size=5000)\n",
    "\n",
    "bbox_list = np.array(bbox_splitter.get_bbox_list())\n",
    "info_list = np.array(bbox_splitter.get_info_list())\n",
    "\n",
    "# Prepare info of selected EOPatches\n",
    "geometry = [Polygon(bbox.get_polygon()) for bbox in bbox_list]\n",
    "idxs_x = [info[\"index_x\"] for info in info_list]\n",
    "idxs_y = [info[\"index_y\"] for info in info_list]\n",
    "idxs = list(range(0, len(idxs_x))) #[info[\"index\"] for info in info_list]\n",
    "\n",
    "bbox_gdf = gpd.GeoDataFrame({\"index\": idxs, \"index_x\": idxs_x, \"index_y\": idxs_y}, crs=country.crs, geometry=geometry)\n",
    "\n",
    "number_of_patches = 20\n",
    "# pick a number of patches\n",
    "patchIDs = random.sample(range(len(bbox_list)), number_of_patches)\n",
    "\n",
    "# Save to shapefile\n",
    "shapefile_name = \"grid_netherlands_500x500.gpkg\"\n",
    "bbox_gdf.to_file(os.path.join(RESULTS_FOLDER, shapefile_name), driver=\"GPKG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display bboxes over country\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.set_title(\"Selected Random Tiles from Netherlands\", fontsize=25)\n",
    "country.plot(ax=ax, facecolor=\"w\", edgecolor=\"b\", alpha=0.5)\n",
    "bbox_gdf.plot(ax=ax, facecolor=\"w\", edgecolor=\"r\", alpha=0.5)\n",
    "\n",
    "for bbox in bbox_gdf.itertuples():\n",
    "    geo = bbox.geometry\n",
    "    ax.text(geo.centroid.x, geo.centroid.y, bbox.index, ha=\"center\", va=\"center\", fontsize=5)\n",
    "\n",
    "# Mark bboxes of selected area\n",
    "bbox_gdf[bbox_gdf.index.isin(patchIDs)].plot(ax=ax, facecolor=\"g\", edgecolor=\"r\", alpha=0.5)\n",
    "\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work Flows\n",
    "\n",
    "#### Access Data\n",
    "\n",
    "Here we define a workflow which gets all the data we will need into the EOPatches.\n",
    "\n",
    "For this first we get the HRVPP data. This is our input data to the model. Then we also get cloud free true color Sentinel 2 data which we need later for some visualization.\n",
    "\n",
    "Then the Ground Truth Data is imported with the `VectorImportTask`. The following tasks already do some data preparation for the ground truth data. The AddLabelTask adds an numeric label. The vectors are then transformed to rasters using this label as the value. Finally the dates in the input data get transformed from a format like YYDOY to a format which records the day since the first of the year.\n",
    "\n",
    "The EOPatch with this data and the applied transformations is then saved for later use."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../media/ml-grassland-classification/access.svg' align='left' height='96px'></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalscript ='''\n",
    "\n",
    "///VERSION=3\n",
    "\n",
    "function setup() {\n",
    "  return {\n",
    "    input: [\"AMPL\",\"EOSD\",\"EOSV\",\"LENGTH\",\"LSLOPE\",\"MAXD\",\"MAXV\",\"MINV\",\"QFLAG\",\"RSLOPE\",\"SOSD\",\"SOSV\",\"SPROD\",\"TPROD\"],\n",
    "    output: [{\n",
    "      id: \"BANDS\",\n",
    "      bands: 14,\n",
    "      sampleType: \"UINT16\"}],\n",
    "  }\n",
    "}\n",
    "//EvaluatePixel function\n",
    "function evaluatePixel(sample) \n",
    "{\n",
    "  return {BANDS: [sample.AMPL, sample.EOSD, sample.EOSV, sample.LENGTH, sample.LSLOPE, sample.MAXD, sample.MAXV, sample.MINV, sample.QFLAG, sample.RSLOPE, sample.SOSD, sample.SOSV, sample.SPROD, sample.TPROD]};\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_color_evalscript='''\n",
    "//VERSION=3\n",
    "let minVal = 0.0;\n",
    "let maxVal = 0.4;\n",
    "\n",
    "let viz = new HighlightCompressVisualizer(minVal, maxVal);\n",
    "\n",
    "function setup() {\n",
    "   return {\n",
    "    input: [\"B04\", \"B03\", \"B02\",\"dataMask\"],\n",
    "    output: [{\n",
    "      id: \"BANDS_TC\",\n",
    "      bands: 4,\n",
    "      sampleType: \"AUTO\"}],\n",
    "  };\n",
    "}\n",
    "\n",
    "function evaluatePixel(samples) {\n",
    "    let val = [samples.B04, samples.B03, samples.B02,samples.dataMask];\n",
    "    return viz.processList(val);\n",
    "}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_true_color = SentinelHubEvalscriptTask(\n",
    "    features=(FeatureType.DATA, \"BANDS_TC\"),\n",
    "    evalscript=true_color_evalscript,\n",
    "    resolution=10,\n",
    "    time_difference=timedelta(days=7),\n",
    "    mosaicking_order='leastCC',\n",
    "    data_collection=DataCollection.SENTINEL2_L2A,\n",
    "    maxcc=0.8,\n",
    "    max_threads=5,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_data = SentinelHubEvalscriptTask(\n",
    "    features=(FeatureType.DATA, \"BANDS\"),\n",
    "    evalscript=evalscript,\n",
    "    resolution=10,\n",
    "    time_difference=None,\n",
    "    data_collection=hrvpp,\n",
    "    max_threads=5,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# SAVING TO OUTPUT (if needed)\n",
    "save = SaveTask(EOPATCH_FOLDER, overwrite_permission=OverwritePermission.OVERWRITE_PATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformDatesTask(EOTask):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def clean_data(self, x_data):\n",
    "        #features = np.concatenate([eopatch.data[\"FEATURES_SAMPLED\"] for eopatch in x_data], axis=1)\n",
    "        # Get shape\n",
    "        \n",
    "        arr = x_data[:, :, :, [1, 5, 10]]\n",
    "        arr = np.nan_to_num(arr, nan=0)\n",
    "        \n",
    "        def yd_to_datetime(yd):\n",
    "            return None if yd == 0 else datetime.strptime(str(yd), '%y%j')            \n",
    "        def count_days(yd):\n",
    "            if yd == -1 or yd is None:\n",
    "                return 0\n",
    "            else:\n",
    "                return (yd - datetime(yd.year, 1, 1)).days\n",
    "            \n",
    "        dates = np.vectorize(yd_to_datetime)(arr)\n",
    "        dates = np.where(dates is None, -1, dates) \n",
    "                    \n",
    "        days = np.vectorize(count_days)(dates)\n",
    "        days = np.where(days == None, 0, days)\n",
    "\n",
    "        x_data[:, :, :, [1, 5, 10]] = days\n",
    "\n",
    "        return x_data\n",
    "    \n",
    "    def execute(self, eopatch):\n",
    "        x_data = eopatch.data[self.features]\n",
    "        x_data = self.clean_data(x_data)\n",
    "        eopatch.data[self.features] = x_data\n",
    "        return eopatch\n",
    "        \n",
    "\n",
    "transformation_task = TransformDatesTask(\"BANDS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_feature = FeatureType.VECTOR_TIMELESS, \"GROUND_TRUTH\"\n",
    "\n",
    "vector_import_task = VectorImportTask(vector_feature, Path(\"../../data/processing/ml-grassland-classification\", \"NL\", \"NL_2020_EC21.shp\"))\n",
    "\n",
    "class AddLabelTask(EOTask):\n",
    "    \"\"\"\n",
    "    Adds a label column with the correct dtype to the vector file\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vector_feature):\n",
    "        self.vector_feature = vector_feature\n",
    "\n",
    "    def execute(self, eopatch):\n",
    "        gpd_df = eopatch.vector_timeless[self.vector_feature]\n",
    "        gpd_df[\"label\"] = gpd_df.EC_hcat_c.astype(float)\n",
    "        eopatch.vector_timeless[self.vector_feature] = gpd_df\n",
    "        return eopatch\n",
    "\n",
    "add_label_task = AddLabelTask(\"GROUND_TRUTH\")\n",
    "\n",
    "rasterization_task = VectorToRasterTask(\n",
    "    vector_feature,\n",
    "    (FeatureType.MASK_TIMELESS, \"LABEL\"),\n",
    "    values_column=\"label\",\n",
    "    raster_shape=(FeatureType.DATA, \"BANDS\"),\n",
    "    raster_dtype=np.uint32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the workflow\n",
    "workflow_nodes = linearly_connect_tasks(\n",
    "     add_data, add_true_color, vector_import_task, add_label_task, rasterization_task, transformation_task, save\n",
    ")\n",
    "workflow = EOWorkflow(workflow_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Time interval for the SH request\n",
    "time_interval = [\"2016-01-05\", \"2021-01-05\"]\n",
    "\n",
    "# Define additional parameters of the workflow\n",
    "input_node = workflow_nodes[0]\n",
    "save_node = workflow_nodes[-1]\n",
    "execution_args = []\n",
    "for idx, bbox in enumerate(bbox_list[patchIDs]):\n",
    "    execution_args.append(\n",
    "        {\n",
    "            input_node: {\"bbox\": bbox, \"time_interval\": time_interval},\n",
    "            save_node: {\"eopatch_folder\": f\"eopatch_{idx}\"},\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Execute the workflow\n",
    "executor = EOExecutor(workflow, execution_args, save_logs=True)\n",
    "executor.run(workers=4)\n",
    "\n",
    "#executor.make_report()\n",
    "\n",
    "failed_ids = executor.get_failed_executions()\n",
    "if failed_ids:\n",
    "    raise RuntimeError(\n",
    "        f\"Execution failed EOPatches with IDs:\\n{failed_ids}\\n\"\n",
    "        f\"For more info check report at {executor.get_report_path()}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Data\n",
    "\n",
    "In this task data is sampled from the ground truth dataset. For this, first the EOPatches created in the previous steps are loaded from disk. Then an Erosion Task is carried out. This erosion task removes pixels on the edges of polygons. This is done because pixels on the edges of polygons are often ambiguous and may contain signals from different land use classes. Doing this erosion thus likely increases the quality and purity of the training data. Afterwards a predetermined fraction of all pixels in the EOPatch are sampled per class. In our case we are sampling a quarter of all pixels. These sampled pixels are then saved again."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../media/ml-grassland-classification/sampling.svg' align='left' height='96px'></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD EXISTING EOPATCHES\n",
    "load = LoadTask(EOPATCH_FOLDER)\n",
    "\n",
    "# erode each class of the reference map\n",
    "erosion = ErosionTask(mask_feature=(FeatureType.MASK_TIMELESS, \"LABEL\", \"LABEL_ERODED\"), disk_radius=1)\n",
    "\n",
    "# SPATIAL SAMPLING\n",
    "# Uniformly sample pixels from patches\n",
    "\n",
    "spatial_sampling = FractionSamplingTask(\n",
    "    features_to_sample=[(FeatureType.DATA, \"BANDS\", \"FEATURES_SAMPLED\"), (FeatureType.MASK_TIMELESS, \"LABEL_ERODED\")],\n",
    "    sampling_feature=(FeatureType.MASK_TIMELESS, \"LABEL_ERODED\"),\n",
    "    fraction=0.25,  # a quarter of points\n",
    "    exclude_values=[0],\n",
    ")\n",
    "\n",
    "save = SaveTask(EOPATCH_SAMPLES_FOLDER, overwrite_permission=OverwritePermission.OVERWRITE_PATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_nodes = linearly_connect_tasks(load, erosion, spatial_sampling, save)\n",
    "workflow = EOWorkflow(workflow_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "execution_args = []\n",
    "for idx in range(len(patchIDs)):\n",
    "    execution_args.append(\n",
    "        {\n",
    "            workflow_nodes[0]: {\"eopatch_folder\": f\"eopatch_{idx}\"},  # load\n",
    "            workflow_nodes[-2]: {\"seed\": 42},  # sampling\n",
    "            workflow_nodes[-1]: {\"eopatch_folder\": f\"eopatch_{idx}\"},  # save\n",
    "        }\n",
    "    )\n",
    "\n",
    "executor = EOExecutor(workflow, execution_args, save_logs=True)\n",
    "executor.run(workers=5)\n",
    "\n",
    "#executor.make_report()\n",
    "\n",
    "failed_ids = executor.get_failed_executions()\n",
    "if failed_ids:\n",
    "    raise RuntimeError(\n",
    "        f\"Execution failed EOPatches with IDs:\\n{failed_ids}\\n\"\n",
    "        f\"For more info check report at {executor.get_report_path()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sampled eopatches\n",
    "sampled_eopatches = []\n",
    "\n",
    "for i in range(len(patchIDs)):\n",
    "    sample_path = EOPATCH_SAMPLES_FOLDER / f\"eopatch_{i}\"\n",
    "    sampled_eopatches.append(EOPatch.load(sample_path, lazy_loading=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation outside eo-learn\n",
    "\n",
    "The sampled EOPatches are split up into a train and test set. The test set will be used for validation and the train set for training the model.\n",
    "\n",
    "This is the part where we have to move away from eo-learn and convert the data into a format usable by the big machine learning libraries. This means that we now have access to the complete training dataset in one, which allows us to do some more pre-processing.\n",
    "\n",
    "The first pre-processing step is cleaning the dataset of null values. These values can not be used in the algorithm and are thus discarded. The second step is the normalization of values using a minMax scaler. This gives all input values the same range, from 0-1. This is done so that values which naturally have a bigger numeric range, do not overproportionally impact the model. Finally the data is randomly undersampled to give both classes, Grassland and No Grassland the same amount of samples. This is called balancing the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../media/ml-grassland-classification/final_prep.svg' align='left' height='96px'></img>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Definition of the train and test patch IDs, take 80 % for train\n",
    "test_ID = rng.choice(len(patchIDs), replace=False, size=int(len(patchIDs)*0.20))\n",
    "test_eopatches = [sampled_eopatches[i] for i in test_ID]\n",
    "train_ID = [i for i in range(len(patchIDs)) if i not in test_ID]\n",
    "train_eopatches = [sampled_eopatches[i] for i in train_ID]\n",
    "\n",
    "\n",
    "# Set the features and the labels for train and test sets\n",
    "features_train = np.concatenate([np.expand_dims(eopatch.data[\"FEATURES_SAMPLED\"][-2], axis=0) for eopatch in train_eopatches], axis=1)\n",
    "labels_train = np.concatenate([eopatch.mask_timeless[\"LABEL_ERODED\"] for eopatch in train_eopatches], axis=0)\n",
    "\n",
    "\n",
    "features_test = np.concatenate([np.expand_dims(eopatch.data[\"FEATURES_SAMPLED\"][-2], axis=0)  for eopatch in test_eopatches], axis=1)\n",
    "labels_test = np.concatenate([eopatch.mask_timeless[\"LABEL_ERODED\"] for eopatch in test_eopatches], axis=0)\n",
    "\n",
    "# Get shape\n",
    "t, w1, h, f = features_train.shape\n",
    "t, w2, h, f = features_test.shape\n",
    "\n",
    "# Reshape to n x m\n",
    "features_train = np.moveaxis(features_train, 0, 2).reshape(w1 * h, t * f)\n",
    "labels_train = labels_train.reshape(w1 * h)\n",
    "features_test = np.moveaxis(features_test, 0, 2).reshape(w2 * h, t * f)\n",
    "labels_test = labels_test.reshape(w2 * h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_null_values(x_data, y_data=None):\n",
    "    \"\"\"Transform the data to dataframe and cleans it\"\"\"\n",
    "    x_df = pd.DataFrame(x_data, columns = [\"AMPL\",\"EOSD\",\"EOSV\",\"LENGTH\",\"LSLOPE\",\"MAXD\",\"MAXV\",\"MINV\",\"QFLAG\",\"RSLOPE\",\"SOSD\",\"SOSV\",\"SPROD\",\"TPROD\"])\n",
    "    if y_data is not None:\n",
    "        y_df = pd.DataFrame(y_data, columns=['LABEL'])\n",
    "        df = x_df.join(y_df)\n",
    "    else:\n",
    "        df = x_df\n",
    "    df = df.dropna()\n",
    "    df = df[~(df[['EOSD', 'SOSD', 'MAXD', 'LENGTH']] == 0).any(axis=1)]\n",
    "    df = df[~(df[['SOSV', 'EOSV', 'MAXV', 'MINV', 'AMPL', 'LSLOPE', 'RSLOPE']] == 32768).any(axis=1)]\n",
    "    df = df[~(df[['SPROD', 'TPROD']] == 65535).any(axis=1)]\n",
    "    return df.drop(['LABEL'], axis=1), df['LABEL']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_train, labels_train = clean_null_values(features_train, labels_train)\n",
    "features_test, labels_test = clean_null_values(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "features_train = scaler.fit_transform(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_classify = [\n",
    "    3302000000  # Grassland\n",
    "    ]\n",
    "labels_train = np.where(np.isin(labels_train, to_classify), 1, 0)\n",
    "labels_test = np.where(np.isin(labels_test, to_classify), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(random_state=0)\n",
    "X_resampled, y_resampled = rus.fit_resample(features_train, labels_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training\n",
    "\n",
    "This step does the actual training. Here we use parameters which we identified in the previous notebook. The model is then saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Set up training classes\n",
    "labels_unique = np.unique(y_resampled)\n",
    "model = lgb.LGBMClassifier(learning_rate=0.1, max_depth=14, num_leaves=300, n_jobs=4, random_state=42)# Set up the model\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, RESULTS_FOLDER / \"model_reduced_classes.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy assessment\n",
    "\n",
    "Here we are classifying the validation data using our newly trained model. We are doing this to visualize the results and to manually calculate accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_path = RESULTS_FOLDER / \"model_reduced_classes.pkl\"\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "# Predict the test labels\n",
    "features_test = scaler.transform(features_test)\n",
    "predicted_labels_test = model.predict(features_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows some basic accuracy metrics, like the F1 score, precision and recall per class and the confusion matrix which tells us where the classification of the model is strong and what its weak points are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_FOLDER/\"label_map.json\") as in_json:\n",
    "    label_map = json.load(in_json)\n",
    "\n",
    "class_labels = np.unique(labels_test)\n",
    "class_names = [label_map[str(class_label)] for class_label in class_labels]\n",
    "mask = np.in1d(predicted_labels_test, labels_test)\n",
    "predictions = predicted_labels_test[mask]\n",
    "true_labels = labels_test[mask]\n",
    "\n",
    "# Extract and display metrics\n",
    "f1_scores = metrics.f1_score(true_labels, predictions, labels=class_labels, average=None)\n",
    "avg_f1_score = metrics.f1_score(true_labels, predictions, average=\"weighted\")\n",
    "recall = metrics.recall_score(true_labels, predictions, labels=class_labels, average=None)\n",
    "precision = metrics.precision_score(true_labels, predictions, labels=class_labels, average=None)\n",
    "accuracy = metrics.accuracy_score(true_labels, predictions)\n",
    "\n",
    "print(\"Classification accuracy {:.1f}%\".format(100 * accuracy))\n",
    "print(\"Classification F1-score {:.1f}%\".format(100 * avg_f1_score))\n",
    "print()\n",
    "print(\"             Class              =  F1  | Recall | Precision\")\n",
    "print(\"         --------------------------------------------------\")\n",
    "for idx, lulctype in enumerate([label_map[str(idx)] for idx in class_labels]):\n",
    "    line_data = (lulctype, f1_scores[idx] * 100, recall[idx] * 100, precision[idx] * 100)\n",
    "    print(\"         * {0:20s} = {1:2.1f} |  {2:2.1f}  | {3:2.1f}\".format(*line_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the plotting function\n",
    "def plot_confusion_matrix(\n",
    "    confusion_matrix,\n",
    "    classes,\n",
    "    normalize=False,\n",
    "    title=\"Confusion matrix\",\n",
    "    cmap=plt.cm.Blues,\n",
    "    ylabel=\"True label\",\n",
    "    xlabel=\"Predicted label\",\n",
    "    filename=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "    if normalize:\n",
    "        normalisation_factor = confusion_matrix.sum(axis=1)[:, np.newaxis] + np.finfo(float).eps\n",
    "        confusion_matrix = confusion_matrix.astype(\"float\") / normalisation_factor\n",
    "\n",
    "    plt.imshow(confusion_matrix, interpolation=\"nearest\", cmap=cmap, vmin=0, vmax=1)\n",
    "    plt.title(title, fontsize=10)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90, fontsize=10)\n",
    "    plt.yticks(tick_marks, classes, fontsize=10)\n",
    "\n",
    "    fmt = \".2f\" if normalize else \"d\"\n",
    "    threshold = confusion_matrix.max() / 2.0\n",
    "    for i, j in itertools.product(range(confusion_matrix.shape[0]), range(confusion_matrix.shape[1])):\n",
    "        plt.text(\n",
    "            j,\n",
    "            i,\n",
    "            format(confusion_matrix[i, j], fmt),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if confusion_matrix[i, j] > threshold else \"black\",\n",
    "            fontsize=12,\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(ylabel, fontsize=10)\n",
    "    plt.xlabel(xlabel, fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "confusion_matrix_gbm = metrics.confusion_matrix(true_labels, predictions)\n",
    "plot_confusion_matrix(\n",
    "    confusion_matrix_gbm,\n",
    "    classes=[label_map[str(idx)] for idx in class_labels],\n",
    "    normalize=True,\n",
    "    ylabel=\"Truth (LAND COVER)\",\n",
    "    xlabel=\"Predicted (GBM)\",\n",
    "    title=\"Confusion matrix\",\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important aspect especially when further developing the model is the importance of different parameters. These parameters are shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature names\n",
    "def plot_feature_importances(model, feature_names):\n",
    "    feature_importance = model.feature_importances_\n",
    "    sorted_idx = feature_importance.argsort()\n",
    "\n",
    "    plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx])\n",
    "    plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
    "    plt.xlabel('Feature importance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(model, [\"AMPL\",\"EOSD\",\"EOSV\",\"LENGTH\",\"LSLOPE\",\"MAXD\",\"MAXV\",\"MINV\",\"QFLAG\",\"RSLOPE\",\"SOSD\",\"SOSV\",\"SPROD\",\"TPROD\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "\n",
    "Here we are now using the trained and validated model to classify data for all 4 years for which we downloaded data. This will give us a visual baseline on how well the model performs and how useful it could be for our usecase.\n",
    "\n",
    "This inference is again done with eo-learn, since a lot of data has to be handled and all of that data is already available in the EOPatches we have constructed earlier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../../media/ml-grassland-classification/inference.svg' align='left' height='96px'></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictPatchTask(EOTask):\n",
    "    \"\"\"\n",
    "    Task to make model predictions on a patch. Provide the model and the feature,\n",
    "    and the output names of labels and scores (optional)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, features_feature, predicted_labels_name, predicted_scores_name=None, scaler=None, dim=None):\n",
    "        self.model = model\n",
    "        self.features_feature = features_feature\n",
    "        self.predicted_labels_name = predicted_labels_name\n",
    "        self.predicted_scores_name = predicted_scores_name\n",
    "        self.scaler = scaler\n",
    "        self.dim = dim\n",
    "\n",
    "    def execute(self, eopatch):\n",
    "        for i, name in enumerate(self.predicted_labels_name):\n",
    "            features = np.expand_dims(eopatch[self.features_feature][i], axis=0)\n",
    "\n",
    "            t, w, h, f = features.shape\n",
    "            features = np.moveaxis(features, 0, 2).reshape(w * h, t * f)\n",
    "            features = scaler.transform(features)\n",
    "\n",
    "            predicted_labels = self.model.predict(features)\n",
    "            predicted_labels = predicted_labels.reshape(w, h)\n",
    "            predicted_labels = predicted_labels[..., np.newaxis]\n",
    "            eopatch[(FeatureType.MASK_TIMELESS, name)] = predicted_labels\n",
    "\n",
    "            if i == self.dim:\n",
    "                predicted_scores = self.model.predict_proba(features)\n",
    "                _, d = predicted_scores.shape\n",
    "                predicted_scores = predicted_scores.reshape(w, h, d)\n",
    "                eopatch[(FeatureType.DATA_TIMELESS, self.predicted_scores_name)] = predicted_scores\n",
    "\n",
    "        return eopatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here set the dim to get the prediction of the year... depends on the years requested the first is 0\n",
    "\n",
    "# LOAD EXISTING EOPATCHES\n",
    "load = LoadTask(EOPATCH_SAMPLES_FOLDER)\n",
    "\n",
    "# PREDICT\n",
    "predictions = [\"LBL_GBM_0\",\"LBL_GBM_1\", \"LBL_GBM_2\", \"LBL_GBM_3\",  \"LBL_GBM_4\"]\n",
    "\n",
    "predict = PredictPatchTask(model, (FeatureType.DATA, \"BANDS\"), predictions , \"SCR_GBM_1\", dim=1)\n",
    "\n",
    "# SAVE\n",
    "save = SaveTask(EOPATCH_SAMPLES_FOLDER, overwrite_permission=OverwritePermission.OVERWRITE_PATCH)\n",
    "\n",
    "# EXPORT TIFF\n",
    "#tiff_location = os.path.join(RESULTS_FOLDER, \"predicted_tiff_1\")\n",
    "#os.makedirs(tiff_location, exist_ok=True)\n",
    "#export_tiff = ExportToTiffTask((FeatureType.MASK_TIMELESS, predictions), tiff_location)\n",
    "\n",
    "workflow_nodes = linearly_connect_tasks(load, predict, save)\n",
    "workflow = EOWorkflow(workflow_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a list of execution arguments for each patch\n",
    "execution_args = []\n",
    "for i in range(len(patchIDs)):\n",
    "    execution_args.append(\n",
    "        {\n",
    "            workflow_nodes[0]: {\"eopatch_folder\": f\"eopatch_{i}\"},\n",
    "            workflow_nodes[2]: {\"eopatch_folder\": f\"eopatch_{i}\"},\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Run the executor\n",
    "executor = EOExecutor(workflow, execution_args)\n",
    "executor.run(workers=1, multiprocess=False)\n",
    "#executor.make_report()\n",
    "\n",
    "failed_ids = executor.get_failed_executions()\n",
    "if failed_ids:\n",
    "    raise RuntimeError(\n",
    "        f\"Execution failed EOPatches with IDs:\\n{failed_ids}\\n\"\n",
    "        f\"For more info check report at {executor.get_report_path()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import from_levels_and_colors\n",
    "lulc_cmap, lulc_norm = from_levels_and_colors([0, 1],['brown','green'], extend=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Draw the Reference map\n",
    "def plot_predicted_vs_ground_truth():\n",
    "    idx = np.random.choice(test_ID)\n",
    "    inspect_size = 100\n",
    "\n",
    "    eopatch = EOPatch.load(os.path.join(EOPATCH_SAMPLES_FOLDER, f\"eopatch_{idx}\"), lazy_loading=True)\n",
    "\n",
    "    w, h = eopatch.mask_timeless[\"LABEL\"].squeeze().shape\n",
    "\n",
    "    w_min = np.random.choice(range(w - inspect_size))\n",
    "    w_max = w_min + inspect_size\n",
    "    h_min = np.random.choice(range(h - inspect_size))\n",
    "    h_max = h_min + inspect_size\n",
    "\n",
    "\n",
    "    ground_truth = np.where(np.isin(eopatch.mask_timeless[\"LABEL\"].squeeze()[w_min:w_max, h_min:h_max], to_classify), 1, 0)\n",
    "\n",
    "    predict = eopatch.mask_timeless[\"LBL_GBM_3\"].squeeze()[w_min:w_max, h_min:h_max]\n",
    "    diff = ground_truth - predict\n",
    "\n",
    "\n",
    "    # Create a colormap with white for 0, gray for -1, and black for 1\n",
    "    cmap_diff = plt.get_cmap('binary', 3)\n",
    "    cmap_diff.set_bad(color='gray')\n",
    "\n",
    "    # Create a new figure with 4 subplots\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(22, 5), gridspec_kw={'width_ratios': [1, 1, 1, 1]})\n",
    "\n",
    "    # Plot the predicted array in the first subplot\n",
    "    axes[0].imshow(predict, cmap=lulc_cmap, norm=lulc_norm, interpolation='nearest', aspect='auto')\n",
    "    axes[0].set_title('Predicted')\n",
    "\n",
    "    # Plot the ground truth array in the second subplot\n",
    "    axes[1].imshow(ground_truth, cmap=lulc_cmap, norm=lulc_norm, interpolation='nearest', aspect='auto')\n",
    "    axes[1].set_title('Ground truth')\n",
    "\n",
    "    # Plot the difference array in the third subplot\n",
    "    axes[2].imshow(diff, cmap=cmap_diff, interpolation='nearest', aspect='auto')\n",
    "    axes[2].set_title('Difference')\n",
    "    # Extract the true color image\n",
    "    image = eopatch.data['BANDS_TC']\n",
    "    # Crop to selected region\n",
    "    # Plot the true color image\n",
    "    axes[3].imshow(image[0, w_min:w_max, h_min:h_max, :])\n",
    "    axes[3].set_title('True Color')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predicted_vs_ground_truth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_progression():\n",
    "    idx = np.random.choice(range(number_of_patches))\n",
    "    inspect_size = 100\n",
    "\n",
    "    eopatch = EOPatch.load(os.path.join(EOPATCH_SAMPLES_FOLDER, f\"eopatch_{idx}\"), lazy_loading=True)\n",
    "\n",
    "    w, h = eopatch.mask_timeless[\"LABEL\"].squeeze().shape\n",
    "\n",
    "    w_min = np.random.choice(range(w - inspect_size))\n",
    "    w_max = w_min + inspect_size\n",
    "    h_min = np.random.choice(range(h - inspect_size))\n",
    "    h_max = h_min + inspect_size\n",
    "\n",
    "\n",
    "    ground_truth = np.where(np.isin(eopatch.mask_timeless[\"LABEL\"].squeeze()[w_min:w_max, h_min:h_max], to_classify), 1, 0)\n",
    "\n",
    "    predict_0 = eopatch.mask_timeless[\"LBL_GBM_0\"].squeeze()[w_min:w_max, h_min:h_max]\n",
    "    predict_1 = eopatch.mask_timeless[\"LBL_GBM_1\"].squeeze()[w_min:w_max, h_min:h_max]\n",
    "    predict_2 = eopatch.mask_timeless[\"LBL_GBM_2\"].squeeze()[w_min:w_max, h_min:h_max]\n",
    "    predict_3 = eopatch.mask_timeless[\"LBL_GBM_3\"].squeeze()[w_min:w_max, h_min:h_max]\n",
    "\n",
    "\n",
    "\n",
    "    # Create a new figure with 4 subplots\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(16, 10), gridspec_kw={'width_ratios': [1, 1, 1, 1]})\n",
    "\n",
    "    # Plot the predicted array in the first subplot\n",
    "    axes[0, 0].imshow(predict_0, cmap=lulc_cmap, norm=lulc_norm, interpolation='nearest', aspect='auto')\n",
    "    axes[0, 0].set_title('Prediction 2017')\n",
    "\n",
    "    # Plot the ground truth array in the second subplot\n",
    "    axes[0, 1].imshow(predict_1, cmap=lulc_cmap, norm=lulc_norm, interpolation='nearest', aspect='auto')\n",
    "    axes[0, 1].set_title('predicted 2018')\n",
    "\n",
    "    # Plot the ground truth array in the second subplot\n",
    "    axes[0, 2].imshow(predict_2, cmap=lulc_cmap, norm=lulc_norm, interpolation='nearest', aspect='auto')\n",
    "    axes[0, 2].set_title('predicted 2019')\n",
    "\n",
    "    # Plot the difference array in the third subplot\n",
    "    axes[0, 3].imshow(predict_3, cmap=lulc_cmap, norm=lulc_norm, interpolation='nearest', aspect='auto')\n",
    "    axes[0, 3].set_title('predicted 2020')\n",
    "    # Extract the true color image\n",
    "    image = eopatch.data['BANDS_TC']\n",
    "    # Crop to selected region\n",
    "    # Plot the true color image\n",
    "    axes[1, 0].imshow(image[0, w_min:w_max, h_min:h_max, :], aspect='auto')\n",
    "    axes[1, 0].set_title('True Color 2017')\n",
    "\n",
    "    axes[1, 1].imshow(image[1, w_min:w_max, h_min:h_max, :], aspect='auto')\n",
    "    axes[1, 1].set_title('True Color 2018')\n",
    "\n",
    "    axes[1, 2].imshow(image[2, w_min:w_max, h_min:h_max, :], aspect='auto')\n",
    "    axes[1, 2].set_title('True Color 2019')\n",
    "\n",
    "    axes[1, 3].imshow(image[3, w_min:w_max, h_min:h_max, :], aspect='auto')\n",
    "    axes[1, 3].set_title('True Color 2020')\n",
    "\n",
    "    # Show the plot\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_progression()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the focus here is showing data preparation and data handling for machine learning tasks, a more thorough analysis will not be carried out. However the notebooks will be made available, so interested people can take a closer look at the data.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "- Data preparation is essential, bad data will lead to bad results\n",
    "- Used tools and workflows depend on the scope\n",
    "- `eo-learn` is well suited for handling large amounts of EO data\n",
    "- This analysis achieved good results, which could be used to monitor grasslands with some further work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
